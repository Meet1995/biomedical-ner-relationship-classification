{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option(\"display.max.rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/parsed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_train = pd.read_parquet(f\"{DATA_PATH}/ner_train.parquet\")\n",
    "df_rel_train = pd.read_parquet(f\"{DATA_PATH}/rel_train.parquet\")\n",
    "\n",
    "df_ner_test = pd.read_parquet(f\"{DATA_PATH}/ner_test.parquet\")\n",
    "df_rel_test = pd.read_parquet(f\"{DATA_PATH}/rel_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(df):\n",
    "    \"\"\"\n",
    "    This function takes a pandas dataframe and adds a new column 'pos' containing the \n",
    "    part-of-speech tags for the 'token' column using the nltk.pos_tag() function. The \n",
    "    part-of-speech tags are grouped by the 'uid', 'contains_rel', and 'sid' columns.\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - df: pandas dataframe with a new column 'pos' containing part-of-speech tags\n",
    "    \n",
    "    \"\"\"\n",
    "    df['pos'] = df.groupby(\n",
    "        ['uid', 'contains_rel', 'sid']\n",
    "    )['token'].transform(lambda s: [x[1] for x in nltk.pos_tag(s)])\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_group_feats(df, window=2):\n",
    "    \"\"\"\n",
    "    This function takes a pandas dataframe, adds new columns for various features such as \n",
    "    whether the token is a title, uppercase, alphabetic, numeric, or contains numbers. It \n",
    "    also sets the index to 'token' and 'tag'. The function then creates new columns for \n",
    "    each feature shifted by the given window size. \n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe\n",
    "    - window: integer, default=2\n",
    "    \n",
    "    Returns:\n",
    "    - df_feats: pandas dataframe with added columns for shifted features\n",
    "    \n",
    "    \"\"\"\n",
    "    df_feats = df.copy()\n",
    "    df_feats['pos'] = df_feats['pos'].astype('string')\n",
    "    df_feats['istitle'] = df_feats['token'].str.istitle().astype('string')\n",
    "    df_feats['isupper'] = df_feats['token'].str.isupper().astype('string')\n",
    "    df_feats['isalpha'] = df_feats['token'].str.isalpha().astype('string')\n",
    "    df_feats['isnumeric'] = df_feats['token'].str.isnumeric().astype('string')\n",
    "    df_feats['containsnumbers'] = df_feats['token'].apply(\n",
    "        lambda x: len(re.findall(r'\\d', x))>0\n",
    "        ).astype('string')\n",
    "    df_feats = df_feats.drop(columns=['uid', 'contains_rel', 'sid'])\n",
    "    df_feats = df_feats.set_index(['token', 'tag'])\n",
    "    ini_cols = df_feats.columns\n",
    "    for s in range(-window, window+1):\n",
    "        if s != 0:\n",
    "            cols = [f\"{s}_{c}\" for c in ini_cols]\n",
    "            shift_df = df_feats[ini_cols].shift(-s)\n",
    "            shift_df.columns = cols\n",
    "            df_feats = pd.concat([df_feats, shift_df], axis=1)\n",
    "    return df_feats\n",
    "\n",
    "\n",
    "def extract_feats(df_ner):\n",
    "    \"\"\"\n",
    "    This function takes a pandas dataframe, gets the part-of-speech tags using the \n",
    "    'get_pos_tags' function, and applies the 'extract_group_feats' function to group the \n",
    "    features for each set of 'uid', 'contains_rel', and 'sid' columns in the dataframe. \n",
    "    \n",
    "    Args:\n",
    "    - df_ner: pandas dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - df_ner: pandas dataframe with added columns for features grouped by 'uid', 'contains_rel', \n",
    "             and 'sid'\n",
    "    \n",
    "    \"\"\"\n",
    "    df_ner = get_pos_tags(df_ner)\n",
    "    return df_ner.groupby(['uid', 'contains_rel', 'sid']).progress_apply(\n",
    "        extract_group_feats\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dependency_graph(token_lst, spacy_pipe):\n",
    "    \"\"\"\n",
    "    This function takes a list of tokens and a spacy pipeline object and returns a \n",
    "    NetworkX graph representing the dependency parse of the input sentence. It uses \n",
    "    spacy_pipe to get the dependencies and then creates an edge list of the dependencies \n",
    "    in the sentence. \n",
    "    \n",
    "    Args:\n",
    "    - token_lst: list of tokens\n",
    "    - spacy_pipe: spacy pipeline object\n",
    "    \n",
    "    Returns:\n",
    "    - g: NetworkX graph object representing the dependency parse of the input sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    doc = spacy_pipe(\" \".join(token_lst))\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((token.text, child.text))\n",
    "    return nx.from_edgelist(edges)\n",
    "\n",
    "\n",
    "def get_sdp(token_lst, s_idx, t_idx, spacy_pipe):\n",
    "    \"\"\"\n",
    "    This function takes a list of tokens, two indices for the source and target tokens, and a \n",
    "    spacy pipeline object. It then creates a dependency graph using the 'get_dependency_graph' \n",
    "    function and finds the shortest dependency path between the two tokens. If there is no path \n",
    "    found, it returns the entire sentence. \n",
    "    \n",
    "    Args:\n",
    "    - token_lst: list of tokens\n",
    "    - s_idx: integer, index of source token\n",
    "    - t_idx: integer, index of target token\n",
    "    - spacy_pipe: spacy pipeline object\n",
    "    \n",
    "    Returns:\n",
    "    - sdp: string representing the shortest dependency path between the source and target \n",
    "           tokens, or the entire sentence if no path was found\n",
    "    \n",
    "    \"\"\"\n",
    "    source = token_lst[s_idx]\n",
    "    target = token_lst[t_idx]\n",
    "    selected_tokens = token_lst[: max(s_idx, t_idx) + 1]\n",
    "    g = get_dependency_graph(selected_tokens, spacy_pipe)\n",
    "    try:\n",
    "        sdp = \" \".join(nx.shortest_path(g, source=source, target=target))\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        sdp = \" \".join(selected_tokens)\n",
    "    return sdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39339/39339 [13:01<00:00, 50.31it/s] \n"
     ]
    }
   ],
   "source": [
    "df_ner_train = extract_feats(df_ner_train)\n",
    "df_ner_train.to_parquet(f\"{DATA_PATH}/ner_train_baseline.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25401/25401 [08:25<00:00, 50.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_ner_test = extract_feats(df_ner_test)\n",
    "df_ner_test.to_parquet(f\"{DATA_PATH}/ner_test_baseline.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36346/36346 [05:14<00:00, 115.64it/s]\n"
     ]
    }
   ],
   "source": [
    "df_rel_train['sdp'] = df_rel_train.progress_apply(\n",
    "    lambda row: get_sdp(\n",
    "        row['text'].split('|'),\n",
    "        int(row['arg1'].split(':')[:-1][0]),\n",
    "        int(row['arg2'].split(':')[:-1][-1]),\n",
    "        nlp\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_train.to_parquet(f\"{DATA_PATH}/rel_train_baseline.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23462/23462 [03:28<00:00, 112.33it/s]\n"
     ]
    }
   ],
   "source": [
    "df_rel_test['sdp'] = df_rel_test.progress_apply(\n",
    "    lambda row: get_sdp(\n",
    "        row['text'].split('|'),\n",
    "        int(row['arg1'].split(':')[:-1][0]),\n",
    "        int(row['arg2'].split(':')[:-1][-1]),\n",
    "        nlp\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_test.to_parquet(f\"{DATA_PATH}/rel_test_baseline.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     She|was|started|on|prophylactic|Oxacillin|to|c...\n",
       "arg1                                                 22:24\n",
       "arg2                                                   5:6\n",
       "label                                          Reason-Drug\n",
       "sid                                                      2\n",
       "uid                                                 100130\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ophthalmic for consulted started on Oxacillin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sdp(\n",
    "        row['text'].split('|'),\n",
    "        int(row['arg1'].split(':')[:-1][0]),\n",
    "        int(row['arg2'].split(':')[:-1][-1]),\n",
    "        nlp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['arg1'].split(':')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ophthalmic', 'Oxacillin')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['text'].split('|')[int(row['arg1'].split(':')[:-1][0])], row['text'].split('|')[int(row['arg2'].split(':')[:-1][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She was started on prophylactic Oxacillin to cover skin flora , and Dermatology was consulted along with Neurology and Ophthalmology for the ophthalmic involvement'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(row['text'].split('|'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
