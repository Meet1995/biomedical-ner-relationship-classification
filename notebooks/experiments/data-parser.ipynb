{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a844a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c4febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities_relationships(file_content):\n",
    "    \"\"\"\n",
    "    Parses out entities and relationships from the entities file.\n",
    "\n",
    "    Args:\n",
    "    file_content (str): A string representing the content of the entities file.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two dictionaries. The first dictionary contains information about entities \n",
    "           found in the file, while the second dictionary contains information about relationships among\n",
    "           entities.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    entities = {}\n",
    "    relationships = {}\n",
    "    for line in file_content.strip().split('\\n'):\n",
    "        if line.strip().startswith(\"#\"):\n",
    "            continue\n",
    "        cols = line.split('\\t')\n",
    "        identifier = cols[0]\n",
    "        if \"Arg1:\" in cols[1]:\n",
    "            relationship_type, arg1, arg2 = cols[1].split()\n",
    "            relationships[identifier] = {\n",
    "                'type': relationship_type,\n",
    "                'arg1': arg1.split(':')[1],\n",
    "                'arg2': arg2.split(':')[1]\n",
    "            }            \n",
    "        else:\n",
    "            split_cols = cols[1].split()\n",
    "            entity_type = split_cols[0]\n",
    "            value = cols[2]\n",
    "            start = split_cols[1]\n",
    "            end = split_cols[-1]\n",
    "            entities[identifier] = {\n",
    "                'type': entity_type,\n",
    "                'start': int(start),\n",
    "                'end': int(end),\n",
    "                'value': value\n",
    "            }\n",
    "    return entities, relationships\n",
    "\n",
    "\n",
    "def get_rel_spans(entities, relationships, txt_str, split_char=\".\"):\n",
    "    \"\"\"\n",
    "    Determines the span of each relationship in a given text.\n",
    "\n",
    "    Args:\n",
    "    entities (dict): A dictionary containing information about entities.\n",
    "    relationships (dict): A dictionary containing information about relationships among entities.\n",
    "    txt_str (str): A string representing the text to be analyzed.\n",
    "    split_char (str, optional): The character to use for splitting text. Defaults to \".\".\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing information about the span of each relationship in the text.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    spans = {}\n",
    "    trim_count = 0\n",
    "    for rel in relationships.keys():\n",
    "        arg1 = entities[relationships[rel]['arg1']]\n",
    "        arg2 = entities[relationships[rel]['arg2']]\n",
    "        min_left = min(arg1['start'], arg2['start'])\n",
    "        max_right = max(arg1['end'], arg2['end'])\n",
    "        # find dots\n",
    "        prev_char = txt_str[:min_left].rfind(split_char)+1\n",
    "        next_char = txt_str[max_right:].find(split_char)+max_right\n",
    "        # see if no exist\n",
    "        assert(prev_char != -1)\n",
    "        assert(next_char != -1)\n",
    "        # trimming very long spans\n",
    "        if min_left - prev_char > 100:\n",
    "            prev_char = min_left - 100\n",
    "            trim_count += 1\n",
    "        if next_char - max_right:\n",
    "            next_char = max_right + 100\n",
    "            trim_count += 1\n",
    "        relationships[rel]['span_start'] = prev_char\n",
    "        relationships[rel]['span_end'] = next_char\n",
    "    # if trim_count > 0:\n",
    "    #     print(trim_count)\n",
    "    return relationships\n",
    "\n",
    "\n",
    "def collate_span_data_with_rels(rel_spans, ents, txt_str):\n",
    "    \"\"\"\n",
    "    Collates information about the span of each relationship in a given text with information about entities.\n",
    "\n",
    "    Args:\n",
    "    rel_spans (dict): A dictionary containing information about the span of each relationship in the text.\n",
    "    ents (dict): A dictionary containing information about entities.\n",
    "    txt_str (str): A string representing the text to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing information about the span of each relationship in the text along with \n",
    "          information about all entities in the text.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    all_data = {}\n",
    "    for i, (rel, rdata_) in enumerate(rel_spans.items()):\n",
    "        span_data = {}\n",
    "\n",
    "        rdata = rdata_.copy()\n",
    "        span = txt_str[rdata['span_start']:rdata['span_end']]\n",
    "        span_data['text'] = span\n",
    "\n",
    "        rdata['arg1_start'] = ents[rdata['arg1']]['start'] - rdata['span_start']\n",
    "        rdata['arg1_end'] = ents[rdata['arg1']]['end'] - rdata['span_start']\n",
    "        rdata['arg2_start'] = ents[rdata['arg2']]['start'] - rdata['span_start']\n",
    "        rdata['arg2_end'] = ents[rdata['arg2']]['end'] - rdata['span_start']\n",
    "        span_data[rel] = rdata\n",
    "        \n",
    "        all_ents = {}\n",
    "        for ent, edata_ in ents.items():\n",
    "            edata = edata_.copy()\n",
    "            if rdata['span_start'] <= edata['start'] and rdata['span_end'] >= edata['end']:\n",
    "                edata['start_pos'] = edata['start'] - rdata['span_start']\n",
    "                edata['end_pos'] = edata['end'] - rdata['span_start']\n",
    "                all_ents[ent] = edata        \n",
    "        span_data['all_entities'] = all_ents    \n",
    "        all_data[i] = span_data        \n",
    "    return all_data\n",
    "\n",
    "\n",
    "def get_uncovered_entity_spans(entities, relations, txt_str, split_char=\".\"):\n",
    "    \"\"\"\n",
    "    Determines the span of each entity in a given text that is not involved in a relationship.\n",
    "\n",
    "    Args:\n",
    "    entities (dict): A dictionary containing information about entities.\n",
    "    relationships (dict): A dictionary containing information about relationships among entities.\n",
    "    txt_str (str): A string representing the text to be analyzed.\n",
    "    split_char (str, optional): The character to use for splitting text. Defaults to \".\".\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing information about the span of each entity in the text that is not involved in\n",
    "          a relationship.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ents_covered = set()\n",
    "    for v in relations.values():\n",
    "        ents_covered = ents_covered.union({v['arg1']}).union({v['arg2']})\n",
    "        \n",
    "    uncovered_dict = {k:v for k, v in entities.items() if k not in ents_covered}\n",
    "    spans = {}\n",
    "    trim_count = 0\n",
    "    for ent, edata in uncovered_dict.items():\n",
    "        min_left = edata['start']\n",
    "        max_right = edata['end']\n",
    "        # find dots\n",
    "        prev_char = txt_str[:min_left].rfind(split_char)+1\n",
    "        next_char = txt_str[max_right:].find(split_char)+max_right\n",
    "        # see if no exist\n",
    "        assert(prev_char != -1)\n",
    "        assert(next_char != -1)\n",
    "        \n",
    "        # trimming very long spans\n",
    "        if min_left - prev_char > 100:\n",
    "            prev_char = min_left - 100\n",
    "            trim_count += 1\n",
    "        if next_char - max_right:\n",
    "            next_char = max_right + 100\n",
    "            trim_count += 1\n",
    "\n",
    "        span = txt_str[prev_char: next_char]\n",
    "        if span not in spans:\n",
    "            spans[span] = {}\n",
    "            \n",
    "        ent_data = {\n",
    "            'start_pos': edata['start'] - prev_char,\n",
    "            'end_pos': edata['end'] - prev_char,\n",
    "            'value': edata['value'],\n",
    "            'type': edata['type']\n",
    "        }\n",
    "        spans[span][ent] = ent_data\n",
    "    # if trim_count > 0:\n",
    "    #     print(trim_count)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def tokenize_preserving_ent(text, entity_mapping):\n",
    "    \"\"\"\n",
    "    Tokenizes text while preserving entity spans.\n",
    "\n",
    "    Args:\n",
    "    text (str): A string representing the text to be tokenized.\n",
    "    entity_mapping (dict): A dictionary containing information about the span of each entity in the text.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens extracted from the input text, while preserving the span of each entity.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    span_locs = [0]\n",
    "    for edata in entity_mapping.values():\n",
    "        span_locs += [edata['start_pos'], edata['end_pos']]\n",
    "\n",
    "    span_locs.append(len(text))\n",
    "    span_locs = sorted(list(set(span_locs)))\n",
    "\n",
    "    start_index = 0\n",
    "    splits = []\n",
    "    for index in span_locs:\n",
    "        split = text[start_index:index].strip()\n",
    "        if split:\n",
    "            splits.append(split)\n",
    "        start_index = index\n",
    "\n",
    "    tokens = []\n",
    "    for s in splits:\n",
    "        tokens += nltk.word_tokenize(s)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tag_index(text, ent_type, ent_start, ent_end, all_ents):\n",
    "    \"\"\"\n",
    "    Determines the index tags of entities in a given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): A string representing the text to be analyzed.\n",
    "    ent_type (str): A string representing the type of the entity.\n",
    "    ent_start (int): An integer representing the starting position of the entity in the text.\n",
    "    ent_end (int): An integer representing the ending position of the entity in the text.\n",
    "    all_ents (dict): A dictionary containing information about all entities in the text.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three elements. The first element is a dictionary containing index tags of entities\n",
    "           in the input text. The second and third elements are integers representing the starting and ending \n",
    "           positions of the entity in the text, respectively.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    index_tag_map = {}\n",
    "    tokens = np.array(tokenize_preserving_ent(text, all_ents))\n",
    "    ent_text = text[ent_start: ent_end]\n",
    "    ent_tokens = nltk.word_tokenize(ent_text)\n",
    "    possible_loc = np.where(tokens==ent_tokens[0])[0]\n",
    "    ent_start_loc = possible_loc[np.argmin(\n",
    "        [abs(ent_start - len(' '.join(tokens[0:x]))) for x in possible_loc]\n",
    "    )]\n",
    "    ent_end_loc = ent_start_loc + len(ent_tokens)\n",
    "    if len(ent_tokens) == 1:\n",
    "        index_tag_map[ent_start_loc] = f\"S-{ent_type}\"\n",
    "    else:\n",
    "        for i in range(ent_start_loc, ent_end_loc):\n",
    "            if i == ent_start_loc:\n",
    "                index_tag_map[i] = f\"B-{ent_type}\"\n",
    "            elif i == ent_end_loc - 1:\n",
    "                index_tag_map[i] = f\"E-{ent_type}\"\n",
    "            else:\n",
    "                index_tag_map[i] = f\"I-{ent_type}\"\n",
    "    return index_tag_map, ent_start_loc, ent_end_loc\n",
    "\n",
    "\n",
    "def get_token_tags(text, all_ents, all_rels=None):\n",
    "    \"\"\"\n",
    "    Determines the tags for each token in a given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): A string representing the text to be analyzed.\n",
    "    all_ents (dict): A dictionary containing information about all entities in the text.\n",
    "    all_rels (dict, optional): A dictionary containing information about relationships among entities. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    pandas.core.frame.DataFrame: A DataFrame containing information about tags for each token in the text.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    tokens = tokenize_preserving_ent(text, all_ents)\n",
    "    tags = ['O'] * len(tokens)\n",
    "    tag_dict = {}\n",
    "    ent_dict = {}\n",
    "    for eid, val in all_ents.items():\n",
    "        tmp, s_idx, e_idx = tag_index(\n",
    "            text, val['type'], val['start_pos'], val['end_pos'], all_ents\n",
    "        )\n",
    "        tag_dict = tag_dict | tmp\n",
    "        ent_dict[eid] = {\n",
    "            'start_idx': s_idx,\n",
    "            'end_idx': e_idx\n",
    "        }\n",
    "    for i, t in tag_dict.items():\n",
    "        tags[i] = t\n",
    "        \n",
    "    if all_rels:\n",
    "        rels = all_rels.copy()\n",
    "        rel_lst = []\n",
    "        for _, rdata in rels.items():\n",
    "            tokens_str = \"|\".join(tokens)\n",
    "            arg1_idx = (f\"{ent_dict[rdata['arg1']]['start_idx']}\"\n",
    "            f\":{ent_dict[rdata['arg1']]['end_idx']}\")\n",
    "            arg2_idx = (f\"{ent_dict[rdata['arg2']]['start_idx']}\"\n",
    "            f\":{ent_dict[rdata['arg2']]['end_idx']}\")\n",
    "            rlabel = rdata['type']\n",
    "            rel_lst.append([tokens_str, arg1_idx, arg2_idx, rlabel])\n",
    "        \n",
    "        ner_df = pd.DataFrame(\n",
    "            list(zip(tokens, tags)), columns=['token', 'tag']\n",
    "        ) \n",
    "        rels_df = pd.DataFrame(rel_lst, columns=['text', 'arg1', 'arg2', 'label'])\n",
    "        return ner_df, rels_df\n",
    "    else:\n",
    "        ner_df = pd.DataFrame(\n",
    "            list(zip(tokens, tags)), columns=['token', 'tag']\n",
    "        ) \n",
    "        return ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2811fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(path):\n",
    "    \"\"\"\n",
    "    The parse_data function takes a file path as an argument and returns two pandas DataFrames. The function reads in text and annotation files from the specified path and processes the data to extract entity and relationship information.\n",
    "\n",
    "    The function returns two DataFrames df_ner and df_rel. df_ner contains tokenized text along with associated entity labels, and df_rel contains tokenized text along with associated relationship labels.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    path: string representing the path to the directory containing the text and annotation files\n",
    "    Returns:\n",
    "\n",
    "    df_ner: a pandas DataFrame containing the tokenized text along with the associated entity labels\n",
    "    df_rel: a pandas DataFrame containing the tokenized text along with the associated relationship labels\n",
    "    \"\"\"\n",
    "    anns = sorted(glob.glob(f\"{path}./*.ann\"))\n",
    "    files = sorted(glob.glob(f\"{path}./*.txt\"))\n",
    "    df1_lst = []\n",
    "    df2_lst = []\n",
    "    for a, f in tqdm(list(zip(anns, files))):\n",
    "        idx = a.split('\\\\')[-1].split('.')[0]\n",
    "        assert idx == f.split('\\\\')[-1].split('.')[0]\n",
    "        with open(a, 'r') as a_f:\n",
    "            ann_str = a_f.read()\n",
    "        with open(f, 'r') as f_f:\n",
    "            txt_str = f_f.read()\n",
    "\n",
    "        ents, rels = parse_entities_relationships(ann_str)\n",
    "        rel_spans = get_rel_spans(ents, rels, txt_str)\n",
    "        collated_data = collate_span_data_with_rels(rel_spans, ents, txt_str)\n",
    "        \n",
    "        all_text_spans = set()\n",
    "        df_ner_lst = []\n",
    "        df_rel_lst = []\n",
    "        for i, data in collated_data.items():\n",
    "            try:\n",
    "                s = data['text']\n",
    "                rel_data = {\n",
    "                    k:v for k,v in data.items() if k not in ['text', 'all_entities']\n",
    "                }\n",
    "                df1, df2 = get_token_tags(s, data['all_entities'], rel_data)\n",
    "                \n",
    "                token_str = \"|\".join(df1['token'].to_list())\n",
    "                if token_str not in all_text_spans:\n",
    "                    df1['sid'] = i\n",
    "                    df1['contains_rel'] = 1\n",
    "                    df_ner_lst.append(df1)\n",
    "                    all_text_spans = all_text_spans.union({token_str})\n",
    "                    \n",
    "                df2['sid'] = i\n",
    "                df_rel_lst.append(df2)\n",
    "            except ValueError:\n",
    "                print(s)\n",
    "                print(data['all_entities'])\n",
    "        \n",
    "        uncovered_ent_spans = get_uncovered_entity_spans(ents, rels, txt_str)\n",
    "        for i, (s, v) in enumerate(uncovered_ent_spans.items()):\n",
    "            try:\n",
    "                df1 = get_token_tags(s, v)\n",
    "                df1['sid'] = i\n",
    "                df1['contains_rel'] = 0\n",
    "                df_ner_lst.append(df1)\n",
    "            except ValueError:\n",
    "                print(s)\n",
    "                print(v)\n",
    "\n",
    "        if len(df_ner_lst) > 0:\n",
    "            df_ner_per_rec = pd.concat(df_ner_lst)\n",
    "            df_ner_per_rec['uid'] = idx\n",
    "            df1_lst.append(df_ner_per_rec)\n",
    "            \n",
    "        if len(df_rel_lst) > 0:\n",
    "            df_rel_per_rec = pd.concat(df_rel_lst)\n",
    "            df_rel_per_rec['uid'] = idx\n",
    "            df2_lst.append(df_rel_per_rec)\n",
    "\n",
    "    df_ner = pd.concat(df1_lst)\n",
    "    df_rel = pd.concat(df2_lst)\n",
    "    df_ner[['token', 'tag']] = df_ner[['token', 'tag']].astype('string')\n",
    "    df_ner[\n",
    "        ['sid', 'contains_rel', 'uid']\n",
    "    ] = df_ner[['sid', 'contains_rel', 'uid']].astype('int')\n",
    "\n",
    "    df_rel[['sid', 'uid']] = df_rel[['sid', 'uid']].astype('int')\n",
    "    df_rel[\n",
    "        ['text', 'arg1', 'arg2', 'label']\n",
    "    ] = df_rel[['text', 'arg1', 'arg2', 'label']].astype('string')\n",
    "\n",
    "    return df_ner, df_rel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a7680a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ca38f95eaa4df8a9433fa5b8b4fc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diastolic CHF and possible restrictive cardiomyopathy\n",
      "(5) AEA/VEA/CAD\n",
      "(6) IDDM with albuminuria\n",
      "(7) Gout on prednisone\n",
      "(8) COPD\n",
      "\n",
      "\n",
      "Social History:\n",
      "- Tobacco: Quit 45 years ago\n",
      "- Alcohol: Denies\n",
      "- Illicits: Denies\n",
      "\n",
      "\n",
      "Fami\n",
      "{'T20': {'type': 'Drug', 'start': 3607, 'end': 3617, 'value': 'prednisone', 'start_pos': 108, 'end_pos': 118}, 'T141': {'type': 'Reason', 'start': 3599, 'end': 3603, 'value': 'Gout', 'start_pos': 100, 'end_pos': 104}, 'T295': {'type': 'Reason', 'start': 3573, 'end': 3577, 'value': 'IDDM', 'start_pos': 74, 'end_pos': 78}, 'T32': {'type': 'Drug', 'start': 3573, 'end': 3574, 'value': 'I', 'start_pos': 74, 'end_pos': 75}}\n",
      "1**]\n",
      "(3) Hypertension\n",
      "(4) Diastolic CHF and possible restrictive cardiomyopathy\n",
      "(5) AEA/VEA/CAD\n",
      "(6) IDDM with albuminuria\n",
      "(7) Gout on prednisone\n",
      "(8) COPD\n",
      "\n",
      "\n",
      "Social History:\n",
      "- Tobacco: Quit 45 years ago\n",
      "- A\n",
      "{'T20': {'type': 'Drug', 'start': 3607, 'end': 3617, 'value': 'prednisone', 'start_pos': 134, 'end_pos': 144}, 'T141': {'type': 'Reason', 'start': 3599, 'end': 3603, 'value': 'Gout', 'start_pos': 126, 'end_pos': 130}, 'T295': {'type': 'Reason', 'start': 3573, 'end': 3577, 'value': 'IDDM', 'start_pos': 100, 'end_pos': 104}, 'T32': {'type': 'Drug', 'start': 3573, 'end': 3574, 'value': 'I', 'start_pos': 100, 'end_pos': 101}}\n"
     ]
    }
   ],
   "source": [
    "df_ner_train, df_rel_train = parse_data(\"../../data/training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d560e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36346, 6), (1435233, 5))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rel_train.shape, df_ner_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d73e9d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "      <th>sid</th>\n",
       "      <th>contains_rel</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multivitamin</td>\n",
       "      <td>S-Drug</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tablet</td>\n",
       "      <td>S-Form</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sig</td>\n",
       "      <td>O</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One</td>\n",
       "      <td>B-Dosage</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(</td>\n",
       "      <td>I-Dosage</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>I-Dosage</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>)</td>\n",
       "      <td>E-Dosage</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tablet</td>\n",
       "      <td>S-Form</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PO</td>\n",
       "      <td>S-Route</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DAILY</td>\n",
       "      <td>B-Frequency</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(</td>\n",
       "      <td>I-Frequency</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Daily</td>\n",
       "      <td>I-Frequency</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>)</td>\n",
       "      <td>E-Frequency</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>115267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token          tag  sid  contains_rel     uid\n",
       "0   multivitamin       S-Drug   79             1  115267\n",
       "1         Tablet       S-Form   79             1  115267\n",
       "2            Sig            O   79             1  115267\n",
       "3              :            O   79             1  115267\n",
       "4            One     B-Dosage   79             1  115267\n",
       "5              (     I-Dosage   79             1  115267\n",
       "6              1     I-Dosage   79             1  115267\n",
       "7              )     E-Dosage   79             1  115267\n",
       "8         Tablet       S-Form   79             1  115267\n",
       "9             PO      S-Route   79             1  115267\n",
       "10         DAILY  B-Frequency   79             1  115267\n",
       "11             (  I-Frequency   79             1  115267\n",
       "12         Daily  I-Frequency   79             1  115267\n",
       "13             )  E-Frequency   79             1  115267"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner_train[(df_ner_train.uid==115267) & (df_ner_train.sid==79) & (df_ner_train.contains_rel==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f106f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36346, 6), (1435395, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rel_train.shape, df_ner_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8567e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_train.to_parquet(\"../../data/parsed_data/rel_train.parquet\")\n",
    "df_ner_train.to_parquet(\"../../data/parsed_data/ner_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d181a3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54c00a7bcbb493a91c41a0fbf72b9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ner_test, df_rel_test = parse_data(\"../../data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e52cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23462, 6), (931604, 5))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rel_test.shape, df_ner_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73aeab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_test.to_parquet(\"../../data/parsed_data/rel_test.parquet\")\n",
    "df_ner_test.to_parquet(\"../../data/parsed_data/ner_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0914f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
