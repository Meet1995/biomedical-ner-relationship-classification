{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2998eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8948db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = \"../../data/test/\"\n",
    "# file_name = \"164726\"\n",
    "base_path = \"../../data/\"\n",
    "file_name = \"100187\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320904de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_str = None\n",
    "with open(base_path + file_name + \".ann\", \"r\") as file:\n",
    "    ann_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2193e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities_relationships(file_content):\n",
    "    \"\"\"\n",
    "    parses out entities and relationships from the entities file\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    relationships = {}\n",
    "\n",
    "    for line in file_content.strip().split('\\n'):\n",
    "        cols = line.split('\\t')\n",
    "        identifier = cols[0]\n",
    "\n",
    "        if \"Arg1:\" in cols[1]:\n",
    "            relationship_type, arg1, arg2 = cols[1].split()\n",
    "            relationships[identifier] = {\n",
    "                'type': relationship_type,\n",
    "                'arg1': arg1.split(':')[1],\n",
    "                'arg2': arg2.split(':')[1]\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            split_cols = cols[1].split()\n",
    "            entity_type = split_cols[0]\n",
    "            value = cols[2]\n",
    "            start = split_cols[1]\n",
    "            end = split_cols[-1]\n",
    "            entities[identifier] = {\n",
    "                'type': entity_type,\n",
    "                'start': int(start),\n",
    "                'end': int(end),\n",
    "                'value': value\n",
    "            }\n",
    "\n",
    "    return entities, relationships\n",
    "\n",
    "\n",
    "def sentence_start_end(sentences, txt_str):\n",
    "    \"\"\"\n",
    "    get starting and ending positions of sentences\n",
    "    \"\"\"\n",
    "    curr_start = 0\n",
    "    sent_positions = []\n",
    "    for i in range(len(sentences)):\n",
    "        sent = sentences[i]\n",
    "        match_idx = txt_str.find(sent, curr_start)\n",
    "        start = match_idx\n",
    "        end = match_idx + len(sent)\n",
    "        curr_start = end\n",
    "        sent_positions.append((start, end))\n",
    "    return sent_positions\n",
    "\n",
    "\n",
    "def update_entity_indices(entities, sent_positions):\n",
    "    \"\"\"\n",
    "    for each entity update its position relative to this sentence\n",
    "    \"\"\"\n",
    "    for entity_key in entities:\n",
    "        entity = entities[entity_key]\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        if entity_key == \"T277\":\n",
    "            print(start, end)\n",
    "        for i, (sent_start, sent_end) in enumerate(sent_positions):\n",
    "            if start >= sent_start and end <= sent_end:\n",
    "                entity['sentence'] = i\n",
    "                entity['sent_start'] = start - sent_start\n",
    "                entity['sent_end'] = end - sent_start\n",
    "                break\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7934f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(m.start(0), m.end(0)) for m in re.finditer(\"7-1\", \"5-7-110010\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be3e8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_str = None\n",
    "with open(base_path + file_name + \".txt\", \"r\") as file:\n",
    "    txt_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "178439db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent, rel = parse_entities_relationships(ann_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e61ca42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(txt_str)\n",
    "sent_pos = sentence_start_end(sentences, txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3259120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "179\n",
      "215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17279, 17328)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    s = sentences[i]\n",
    "    if \"Pantoprazole\" in s:\n",
    "        print(i)\n",
    "sent_pos[215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "298be158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17279 17291\n"
     ]
    }
   ],
   "source": [
    "new_ent = update_entity_indices(ent, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d83b3552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Drug',\n",
       " 'start': 17279,\n",
       " 'end': 17291,\n",
       " 'value': 'Pantoprazole',\n",
       " 'sentence': 215,\n",
       " 'sent_start': 0,\n",
       " 'sent_end': 12}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ent['T277']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd93c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_covered = set(\n",
    "    itertools.chain.from_iterable(([[r['arg1'], r['arg2']] for k, r in rel.items()]))\n",
    "    )\n",
    "uncovered = set(ent.keys()) - ents_covered\n",
    "\n",
    "uncovered_dict = {k: v for k, v in ent.items() if k in uncovered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = update_entity_indices(uncovered_dict, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59403e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_updated = update_entity_indices(ent, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c58714",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = get_relationship_spans(ent_updated, rel, txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686abe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relationship_spans(entities, relationships, txt_str):\n",
    "    \"\"\"\n",
    "    get relationship spans\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(txt_str)\n",
    "    entities = update_entity_indices(entities, sentence_start_end(sentences, txt_str))\n",
    "\n",
    "    relationship_spans = {}\n",
    "\n",
    "    for rel_id, relationship in relationships.items():\n",
    "        arg1 = entities[relationship['arg1']]\n",
    "        arg2 = entities[relationship['arg2']]\n",
    "        if arg1['sentence'] <= arg2['sentence']:\n",
    "            sent_start = arg1['sentence']\n",
    "            sent_end = arg2['sentence']\n",
    "            arg1_start = arg1['sent_start']\n",
    "            arg1_end = arg1['sent_end']\n",
    "            offset = len(' '.join(sentences[sent_start: sent_end]))\n",
    "            arg2_start = offset + arg2['sent_start']\n",
    "            arg2_end = offset + arg2['sent_end']\n",
    "        else:\n",
    "            sent_start = arg2['sentence']\n",
    "            sent_end = arg1['sentence']\n",
    "            arg2_start = arg2['sent_start']\n",
    "            arg2_end = arg2['sent_end']\n",
    "            offset = len(' '.join(sentences[sent_start: sent_end]))\n",
    "            arg1_start = offset + arg1['sent_start']\n",
    "            arg1_end = offset + arg1['sent_end']\n",
    "        \n",
    "        span = ' '.join(sentences[sent_start:sent_end+1])\n",
    "        \n",
    "#         print(len(span[arg1_start:arg1_end]), len(arg1['value']))\n",
    "#         print(span[arg1_start:arg1_end], arg1['value'])\n",
    "#         print(len(span[arg2_start:arg2_end]), len(arg2['value']))\n",
    "#         print(span[arg2_start:arg2_end], arg2['value'])\n",
    "#         assert (\n",
    "#             (span[arg1_start:arg1_end] == arg1['value']) and \n",
    "#             (span[arg2_start:arg2_end] == arg2['value'])\n",
    "#         )\n",
    "\n",
    "        if span not in relationship_spans:\n",
    "            relationship_spans[span] = {\n",
    "                'relationships': {},\n",
    "                'all_entities': {}\n",
    "            }\n",
    "\n",
    "        relationship_spans[span]['relationships'][rel_id] = {\n",
    "            'type': relationship['type'],\n",
    "            'arg1': relationship['arg1'],\n",
    "            'arg1_start': arg1_start,\n",
    "            'arg1_end': arg1_end,\n",
    "            'arg2': relationship['arg2'],\n",
    "            'arg2_start': arg2_start,\n",
    "            'arg2_end': arg2_end\n",
    "        }\n",
    "\n",
    "        if relationship['arg1'] not in relationship_spans[span]['all_entities']:\n",
    "            ent_data = {\n",
    "                'start_pos': arg1_start,\n",
    "                'end_pos': arg1_end,\n",
    "                'text': arg1['value'],\n",
    "                'type': arg1['type']\n",
    "            }\n",
    "            relationship_spans[span]['all_entities'][relationship['arg1']] = ent_data\n",
    "        \n",
    "        if relationship['arg2'] not in relationship_spans[span]['all_entities']:\n",
    "            ent_data = {\n",
    "                'start_pos': arg2_start,\n",
    "                'end_pos': arg2_end,\n",
    "                'text': arg2['value'],\n",
    "                'type': arg2['type']\n",
    "            }\n",
    "            relationship_spans[span]['all_entities'][relationship['arg2']] = ent_data\n",
    "    return relationship_spans\n",
    "\n",
    "\n",
    "def get_ent_tags(text, ent_type, ent_text, ent_loc):\n",
    "    index_tag_map = {}\n",
    "    tokens = np.array(nltk.word_tokenize(text))\n",
    "    ent_tokens = nltk.word_tokenize(ent_text)\n",
    "    possible_loc = np.where(tokens==ent_tokens[0])[0]\n",
    "    ent_start_loc = possible_loc[np.argmin(\n",
    "        [abs(ent_loc - len(' '.join(tokens[0:x]))) for x in possible_loc]\n",
    "    )]\n",
    "    ent_end_loc = ent_start_loc + len(ent_tokens) - 1\n",
    "    if len(ent_tokens) == 1:\n",
    "        index_tag_map[ent_start_loc] = f\"S-{ent_type}\"\n",
    "    else:\n",
    "        for i in range(ent_start_loc, ent_end_loc):\n",
    "            if i == ent_start_loc:\n",
    "                index_tag_map[i] = f\"B-{ent_type}\"\n",
    "            elif i == ent_end_loc - 1:\n",
    "                index_tag_map[i] = f\"E-{ent_type}\"\n",
    "            else:\n",
    "                index_tag_map[i] = f\"I-{ent_type}\"\n",
    "    return index_tag_map, ent_start_loc, ent_end_loc\n",
    "\n",
    "\n",
    "def get_token_tags(text, all_ents, all_rels=None):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = ['O'] * len(tokens)\n",
    "    tag_dict = {}\n",
    "    ent_dict = {}\n",
    "    for eid, val in all_ents.items():\n",
    "        tmp, s_idx, e_idx = get_ent_tags(\n",
    "            text, val['type'], val['text'], val['start_pos']\n",
    "        )\n",
    "        tag_dict = tag_dict | tmp\n",
    "        ent_dict[eid] = {\n",
    "            'start_idx': s_idx,\n",
    "            'end_idx': e_idx\n",
    "        }\n",
    "    for i, t in tag_dict.items():\n",
    "        tags[i] = t\n",
    "        \n",
    "    if all_rels:\n",
    "        rels = all_rels.copy()\n",
    "        for _, rdata in rels.items():\n",
    "            rdata['arg1_start'] = ent_dict[rdata['arg1']]['start_idx']\n",
    "            rdata['arg1_end'] = ent_dict[rdata['arg1']]['end_idx']\n",
    "            rdata['arg2_start'] = ent_dict[rdata['arg2']]['start_idx']\n",
    "            rdata['arg2_end'] = ent_dict[rdata['arg2']]['end_idx']\n",
    "        return list(zip(tokens, tags)), rels\n",
    "    else:\n",
    "        return list(zip(tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd290638",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in spans.items():\n",
    "    print(f\"{k}\\n\")\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc940b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57179199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 'Aggressive diuresis\\nwith IV lasix and diuril were unsuccessful so CVVH was\\ninitiated.'\n",
    "# get_token_tags(t, spans[t]['all_entities'], spans[t]['relationships'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences_without_relationships(sentences, sent_positions, entities, relationships, relationship_spans):\n",
    "    processed_sentences = {entities[rel['arg1']]['sentence'] for rel in relationships.values()}\n",
    "    processed_sentences.update({entities[rel['arg2']]['sentence'] for rel in relationships.values()})\n",
    "    for i, (sent_start, sent_end) in enumerate(sent_positions):\n",
    "        if i not in processed_sentences:\n",
    "            entities_in_sent = [e for e in entities.values() if e['sentence'] == i]\n",
    "            if len(entities_in_sent) > 1:\n",
    "                span = sentences[i]\n",
    "\n",
    "                if span not in relationship_spans:\n",
    "                    relationship_spans[span] = {\n",
    "                        'relationships': [],\n",
    "                        'entities': {}\n",
    "                    }\n",
    "\n",
    "                for j, (e1, e2) in enumerate(zip(entities_in_sent[:-1], entities_in_sent[1:])):\n",
    "                    print(e1['id'], e2['id'])\n",
    "                    relationship_spans[span]['relationships'].append({\n",
    "                        'id': f'NOREL{i}_{j}',\n",
    "                        'type': 'NOREL',\n",
    "                        'arg1': e1['id'],\n",
    "                        'arg2': e2['id']\n",
    "                    })\n",
    "\n",
    "                for e in entities_in_sent:\n",
    "                    relationship_spans[span]['entities'][e['id']] = e\n",
    "    return relationship_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "norels = process_sentences_without_relationships(sentences, sent_pos, ent, rel, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184012e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41690652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "26128e264d228505edcdd079c0afd2b8cc37c9a8b8aeae9abc308e3b38af8b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
