{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2998eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e38c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities_relationships(file_content):\n",
    "    \"\"\"\n",
    "    parses out entities and relationships from the entities file\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    relationships = {}\n",
    "\n",
    "    for line in file_content.strip().split('\\n'):\n",
    "        cols = line.split('\\t')\n",
    "        identifier = cols[0]\n",
    "\n",
    "        if \"Arg1:\" in cols[1]:\n",
    "            relationship_type, arg1, arg2 = cols[1].split()\n",
    "            relationships[identifier] = {\n",
    "                'type': relationship_type,\n",
    "                'arg1': arg1.split(':')[1],\n",
    "                'arg2': arg2.split(':')[1]\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            split_cols = cols[1].split()\n",
    "            entity_type = split_cols[0]\n",
    "            value = cols[2]\n",
    "            start = split_cols[1]\n",
    "            end = split_cols[-1]\n",
    "            entities[identifier] = {\n",
    "                'type': entity_type,\n",
    "                'start': int(start),\n",
    "                'end': int(end),\n",
    "                'value': value\n",
    "            }\n",
    "    return entities, relationships\n",
    "\n",
    "\n",
    "def sentence_start_end(txt_str):\n",
    "    \"\"\"\n",
    "    get starting and ending positions of sentences\n",
    "    \"\"\"\n",
    "    sent_tokenizer = PunktSentenceTokenizer(txt_str)\n",
    "    sentences = sent_tokenizer.tokenize(txt_str)\n",
    "    curr_start = 0\n",
    "    sent_positions = []\n",
    "    for i in range(len(sentences)):\n",
    "        sent = sentences[i]\n",
    "        match_idx = txt_str.find(sent, curr_start)\n",
    "        start = match_idx\n",
    "        end = match_idx + len(sent)\n",
    "        curr_start = end\n",
    "        sent_positions.append((start, end))\n",
    "    return sent_positions, sentences\n",
    "\n",
    "\n",
    "def update_entity_indices(entities, sent_positions):\n",
    "    \"\"\"\n",
    "    for each entity update its position relative to this sentence\n",
    "    \"\"\"\n",
    "    for entity_key in entities:\n",
    "        entity = entities[entity_key]\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        for i, (sent_start, sent_end) in enumerate(sent_positions):\n",
    "            if start >= sent_start and end <= sent_end:\n",
    "                entity['sentence'] = i\n",
    "                entity['sent_start'] = start - sent_start\n",
    "                entity['sent_end'] = end - sent_start\n",
    "                break\n",
    "        assert 'sentence' in entity, f\"{entity_key}\"\n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_relationship_spans(entities, relationships, txt_str):\n",
    "    \"\"\"\n",
    "    get relationship spans\n",
    "    \"\"\"\n",
    "    entities = update_entity_indices(entities, sentence_start_end(txt_str)[0])\n",
    "\n",
    "    relationship_spans = {}\n",
    "\n",
    "    for rel_id, relationship in relationships.items():\n",
    "        arg1 = entities[relationship['arg1']]\n",
    "        arg2 = entities[relationship['arg2']]\n",
    "        if arg1['sentence'] <= arg2['sentence']:\n",
    "            sent_start = arg1['sentence']\n",
    "            sent_end = arg2['sentence']\n",
    "            arg1_start = arg1['sent_start']\n",
    "            arg1_end = arg1['sent_end']\n",
    "            offset = len(' '.join(sentences[sent_start: sent_end]))\n",
    "            arg2_start = offset + arg2['sent_start']\n",
    "            arg2_end = offset + arg2['sent_end']\n",
    "        else:\n",
    "            sent_start = arg2['sentence']\n",
    "            sent_end = arg1['sentence']\n",
    "            arg2_start = arg2['sent_start']\n",
    "            arg2_end = arg2['sent_end']\n",
    "            offset = len(' '.join(sentences[sent_start: sent_end]))\n",
    "            arg1_start = offset + arg1['sent_start']\n",
    "            arg1_end = offset + arg1['sent_end']\n",
    "        \n",
    "        span = ' '.join(sentences[sent_start:sent_end+1])\n",
    "        \n",
    "#         print(len(span[arg1_start:arg1_end]), len(arg1['value']))\n",
    "#         print(span[arg1_start:arg1_end], arg1['value'])\n",
    "#         print(len(span[arg2_start:arg2_end]), len(arg2['value']))\n",
    "#         print(span[arg2_start:arg2_end], arg2['value'])\n",
    "#         assert (\n",
    "#             (span[arg1_start:arg1_end] == arg1['value']) and \n",
    "#             (span[arg2_start:arg2_end] == arg2['value'])\n",
    "#         )\n",
    "\n",
    "        if span not in relationship_spans:\n",
    "            relationship_spans[span] = {\n",
    "                'relationships': {},\n",
    "                'all_entities': {}\n",
    "            }\n",
    "\n",
    "        relationship_spans[span]['relationships'][rel_id] = {\n",
    "            'type': relationship['type'],\n",
    "            'arg1': relationship['arg1'],\n",
    "            'arg1_start': arg1_start,\n",
    "            'arg1_end': arg1_end,\n",
    "            'arg2': relationship['arg2'],\n",
    "            'arg2_start': arg2_start,\n",
    "            'arg2_end': arg2_end\n",
    "        }\n",
    "        if relationship['arg1'] not in relationship_spans[span]['all_entities']:\n",
    "            ent_data = {\n",
    "                'start_pos': arg1_start,\n",
    "                'end_pos': arg1_end,\n",
    "                'text': arg1['value'],\n",
    "                'type': arg1['type']\n",
    "            }\n",
    "            relationship_spans[span]['all_entities'][relationship['arg1']] = ent_data\n",
    "        \n",
    "        if relationship['arg2'] not in relationship_spans[span]['all_entities']:\n",
    "            ent_data = {\n",
    "                'start_pos': arg2_start,\n",
    "                'end_pos': arg2_end,\n",
    "                'text': arg2['value'],\n",
    "                'type': arg2['type']\n",
    "            }\n",
    "            relationship_spans[span]['all_entities'][relationship['arg2']] = ent_data\n",
    "    return relationship_spans\n",
    "\n",
    "\n",
    "def get_uncovered_entity_spans(entities_map, relations, txt_str):\n",
    "    updated_entities = update_entity_indices(\n",
    "        entities_map, sentence_start_end(txt_str)[0]\n",
    "    )\n",
    "    ents_covered = set()\n",
    "    for v in relations.values():\n",
    "        ents_covered = ents_covered.union({v['arg1']}).union({v['arg2']})\n",
    "        \n",
    "    uncovered_dict = {k:v for k, v in updated_entities.items() if k not in ents_covered}\n",
    "\n",
    "    spans = {}\n",
    "    for ent, loc in uncovered_dict.items():\n",
    "        sent = sentences[loc['sentence']]\n",
    "        if sent not in spans:\n",
    "            spans[sent] = {}\n",
    "            \n",
    "        ent_data = {\n",
    "            'start_pos': loc['sent_start'],\n",
    "            'end_pos': loc['sent_end'],\n",
    "            'text': loc['value'],\n",
    "            'type': loc['type']\n",
    "        }\n",
    "        spans[sent][ent] = ent_data\n",
    "    return spans\n",
    "\n",
    "\n",
    "def tag_index(text, ent_type, ent_text, ent_loc):\n",
    "    index_tag_map = {}\n",
    "    tokens = np.array(nltk.word_tokenize(text))\n",
    "    ent_tokens = nltk.word_tokenize(ent_text)\n",
    "    possible_loc = np.where(tokens==ent_tokens[0])[0]\n",
    "    ent_start_loc = possible_loc[np.argmin(\n",
    "        [abs(ent_loc - len(' '.join(tokens[0:x]))) for x in possible_loc]\n",
    "    )]\n",
    "    ent_end_loc = ent_start_loc + len(ent_tokens) - 1\n",
    "    if len(ent_tokens) == 1:\n",
    "        index_tag_map[ent_start_loc] = f\"S-{ent_type}\"\n",
    "    else:\n",
    "        for i in range(ent_start_loc, ent_end_loc):\n",
    "            if i == ent_start_loc:\n",
    "                index_tag_map[i] = f\"B-{ent_type}\"\n",
    "            elif i == ent_end_loc - 1:\n",
    "                index_tag_map[i] = f\"E-{ent_type}\"\n",
    "            else:\n",
    "                index_tag_map[i] = f\"I-{ent_type}\"\n",
    "    return index_tag_map, ent_start_loc, ent_end_loc\n",
    "\n",
    "\n",
    "def get_token_tags(text, all_ents, all_rels=None):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = ['O'] * len(tokens)\n",
    "    tag_dict = {}\n",
    "    ent_dict = {}\n",
    "    for eid, val in all_ents.items():\n",
    "        tmp, s_idx, e_idx = tag_index(\n",
    "            text, val['type'], val['text'], val['start_pos']\n",
    "        )\n",
    "        tag_dict = tag_dict | tmp\n",
    "        ent_dict[eid] = {\n",
    "            'start_idx': s_idx,\n",
    "            'end_idx': e_idx\n",
    "        }\n",
    "    for i, t in tag_dict.items():\n",
    "        tags[i] = t\n",
    "        \n",
    "    if all_rels:\n",
    "        rels = all_rels.copy()\n",
    "        rel_lst = []\n",
    "        for _, rdata in rels.items():\n",
    "            tokens_str = \"|\".join(tokens)\n",
    "            arg1_idx = (f\"{ent_dict[rdata['arg1']]['start_idx']}\"\n",
    "            f\":{ent_dict[rdata['arg1']]['end_idx']}\")\n",
    "            arg2_idx = (f\"{ent_dict[rdata['arg1']]['start_idx']}\"\n",
    "            f\":{ent_dict[rdata['arg1']]['end_idx']}\")\n",
    "            rel_lst.append([tokens_str, arg1_idx, arg2_idx])\n",
    "        \n",
    "        ner_df = pd.DataFrame(\n",
    "            list(zip(tokens, tags)), columns=['token', 'tag']\n",
    "        ) \n",
    "        rels_df = pd.DataFrame(rel_lst, columns=['text', 'arg1', 'arg2'])\n",
    "        return ner_df, rels_df\n",
    "    else:\n",
    "        ner_df = pd.DataFrame(\n",
    "            list(zip(tokens, tags)), columns=['token', 'tag']\n",
    "        ) \n",
    "        return ner_df\n",
    "\n",
    "def parse_data(path):\n",
    "    anns = sorted(glob.glob(f\"{path}./*.ann\"))\n",
    "    files = sorted(glob.glob(f\"{path}./*.txt\"))\n",
    "    df1_lst = []\n",
    "    df2_lst = []\n",
    "    for a, f in zip(anns, files):\n",
    "        idx = a.split('\\\\')[-1].split('.')[0]\n",
    "        assert idx == f.split('\\\\')[-1].split('.')[0]\n",
    "        with open(a, 'r') as a_f:\n",
    "            ann_str = a_f.read()\n",
    "        with open(f, 'r') as f_f:\n",
    "            txt_str = f_f.read()\n",
    "        try:\n",
    "            ent, rel = parse_entities_relationships(ann_str)\n",
    "            rel_spans = get_relationship_spans(ent, rel, txt_str)\n",
    "        except Exception as e:\n",
    "            print(idx)\n",
    "            raise ValueError\n",
    "        df_ner_lst = []\n",
    "        df_rel_lst = []\n",
    "        tmp = []\n",
    "        for i, (s, v) in enumerate(rel_spans.items()):\n",
    "            try:\n",
    "                df1, df2 = get_token_tags(s, v['all_entities'], v['relationships'])\n",
    "                df1['sid'] = i\n",
    "                df1['contains_rel'] = 1\n",
    "                df2['sid'] = i\n",
    "                df_ner_lst.append(df1)\n",
    "                df_rel_lst.append(df2)\n",
    "            except:\n",
    "                print(f\"{idx}: {s}\")\n",
    "                tmp.append((s, v))\n",
    "        uncovered_ent_spans = get_uncovered_entity_spans(ent, rel, txt_str)\n",
    "        for i, (s, v) in enumerate(uncovered_ent_spans.items()):\n",
    "            try:\n",
    "                df1 = get_token_tags(s, v)\n",
    "                df1['sid'] = i\n",
    "                df1['contains_rel'] = 0\n",
    "                df_ner_lst.append(df1)\n",
    "            except:\n",
    "                print(f\"{idx}: {s}\")\n",
    "                tmp.append((s, v))\n",
    "        df_ner_per_rec = pd.concat(df_ner_lst)\n",
    "        df_ner_per_rec['uid'] = idx\n",
    "        df1_lst.append(df_ner_per_rec)\n",
    "        df_rel_per_rec = pd.concat(df_rel_lst)\n",
    "        df_rel_per_rec['uid'] = idx\n",
    "        df2_lst.append(df_rel_per_rec)\n",
    "\n",
    "    df_ner = pd.concat(df1_lst)\n",
    "    df_rel = pd.concat(df2_lst)\n",
    "    return df_ner, df_rel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ffec789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = \"../../data/test/\"\n",
    "# file_name = \"164726\"\n",
    "base_path = \"../../data/training/\"\n",
    "file_name = \"101215\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9e57d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Potassium Chloride 20 mEq Tab Sust.Rel. Particle/Crystal Sig:\n",
      "One (1) Tab Sust\n"
     ]
    }
   ],
   "source": [
    "for i in range(ent['T59']['end'], 1000000):\n",
    "    if txt_str[i] == '.':\n",
    "        end_loc = i\n",
    "        break\n",
    "\n",
    "for i in range(ent['T57']['start'], 0, -1):\n",
    "    if txt_str[i] == '.':\n",
    "        start_loc = i    \n",
    "        break\n",
    "print(txt_str[start_loc: end_loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9bcf5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5670"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent['T59']['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "556b246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_str[5692] == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49539db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tab Sust.Rel. Particle/Crystal Sig:\\nOne (1) Tab Sust.Rel. Particle/Crystal PO once a day for 7 days.\\nDisp:*7 Tab Sust.Rel. Particle/Crystal(s)* Refills:*0*\\n7. Pantoprazole 40 mg Tablet, Delayed Release (E.C.) Sig: One\\n(1) Tablet, Delayed Release (E.C.) PO Q24H (every 24 hours).\\nDisp:*30 Tablet, Delayed Release (E.C.)(s)* Refills:*0*\\n8. Labetalol 200 mg Tablet Sig: Four (4) Tablet PO TID (3 times\\na day).\\nDisp:*360 Tablet(s)* Refills:*0*\\n\\n\\nDischarge Disposition:\\nExtended Care\\n\\nFacility:\\n[**Hospital3 1107**] [**Hospital **] Hospital - [**Location (un) 38**]\\n\\nDischarge Diagnosis:\\nType A Aortic Dissection, Aortic Insufficiency - s/p Repair\\nPostoperative Bleeding/Pericardial Effusion - s/p Re-exploration\\nPostoperative Toxic-Metabolic Encephalopathy\\nHypertension\\nDyslipidemia\\n\\n\\nDischarge Condition:\\nGood\\n\\n\\nDischarge Instructions:\\n1)Please shower daily. No baths. Pat dry incisions, do not rub.\\n2)Avoid creams and lotions to surgical incisions.\\n3)Call cardiac surgeon if there is concern for wound infection.\\n4)No lifting more t'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_str[ent['T59']['start']: ent['T59']['end']+1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "320904de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_str = None\n",
    "with open(base_path + file_name + \".ann\", \"r\") as file:\n",
    "    ann_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be3e8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_str = None\n",
    "with open(base_path + file_name + \".txt\", \"r\") as file:\n",
    "    txt_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cb189ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = sentence_start_end(txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a97a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent, rel = parse_entities_relationships(ann_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "751f0d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R1': {'type': 'Reason-Drug', 'arg1': 'T2', 'arg2': 'T3'},\n",
       " 'R2': {'type': 'Route-Drug', 'arg1': 'T4', 'arg2': 'T3'},\n",
       " 'R4': {'type': 'Reason-Drug', 'arg1': 'T8', 'arg2': 'T7'},\n",
       " 'R5': {'type': 'Reason-Drug', 'arg1': 'T9', 'arg2': 'T7'},\n",
       " 'R8': {'type': 'Strength-Drug', 'arg1': 'T15', 'arg2': 'T14'},\n",
       " 'R9': {'type': 'Frequency-Drug', 'arg1': 'T16', 'arg2': 'T14'},\n",
       " 'R10': {'type': 'Strength-Drug', 'arg1': 'T19', 'arg2': 'T18'},\n",
       " 'R11': {'type': 'Frequency-Drug', 'arg1': 'T21', 'arg2': 'T20'},\n",
       " 'R12': {'type': 'Strength-Drug', 'arg1': 'T23', 'arg2': 'T22'},\n",
       " 'R13': {'type': 'Form-Drug', 'arg1': 'T24', 'arg2': 'T22'},\n",
       " 'R14': {'type': 'Dosage-Drug', 'arg1': 'T25', 'arg2': 'T22'},\n",
       " 'R15': {'type': 'Form-Drug', 'arg1': 'T26', 'arg2': 'T22'},\n",
       " 'R16': {'type': 'Route-Drug', 'arg1': 'T27', 'arg2': 'T22'},\n",
       " 'R17': {'type': 'Frequency-Drug', 'arg1': 'T28', 'arg2': 'T22'},\n",
       " 'R18': {'type': 'Form-Drug', 'arg1': 'T30', 'arg2': 'T29'},\n",
       " 'R19': {'type': 'Strength-Drug', 'arg1': 'T31', 'arg2': 'T29'},\n",
       " 'R20': {'type': 'Form-Drug', 'arg1': 'T32', 'arg2': 'T29'},\n",
       " 'R21': {'type': 'Dosage-Drug', 'arg1': 'T33', 'arg2': 'T29'},\n",
       " 'R22': {'type': 'Route-Drug', 'arg1': 'T34', 'arg2': 'T29'},\n",
       " 'R23': {'type': 'Frequency-Drug', 'arg1': 'T35', 'arg2': 'T29'},\n",
       " 'R24': {'type': 'Strength-Drug', 'arg1': 'T37', 'arg2': 'T36'},\n",
       " 'R25': {'type': 'Form-Drug', 'arg1': 'T38', 'arg2': 'T36'},\n",
       " 'R26': {'type': 'Dosage-Drug', 'arg1': 'T39', 'arg2': 'T36'},\n",
       " 'R27': {'type': 'Form-Drug', 'arg1': 'T40', 'arg2': 'T36'},\n",
       " 'R28': {'type': 'Route-Drug', 'arg1': 'T41', 'arg2': 'T36'},\n",
       " 'R30': {'type': 'Strength-Drug', 'arg1': 'T44', 'arg2': 'T43'},\n",
       " 'R31': {'type': 'Form-Drug', 'arg1': 'T45', 'arg2': 'T43'},\n",
       " 'R32': {'type': 'Dosage-Drug', 'arg1': 'T46', 'arg2': 'T43'},\n",
       " 'R33': {'type': 'Form-Drug', 'arg1': 'T47', 'arg2': 'T43'},\n",
       " 'R34': {'type': 'Route-Drug', 'arg1': 'T48', 'arg2': 'T43'},\n",
       " 'R35': {'type': 'Frequency-Drug', 'arg1': 'T49', 'arg2': 'T43'},\n",
       " 'R36': {'type': 'Strength-Drug', 'arg1': 'T51', 'arg2': 'T50'},\n",
       " 'R37': {'type': 'Form-Drug', 'arg1': 'T52', 'arg2': 'T50'},\n",
       " 'R38': {'type': 'Dosage-Drug', 'arg1': 'T53', 'arg2': 'T50'},\n",
       " 'R39': {'type': 'Form-Drug', 'arg1': 'T54', 'arg2': 'T50'},\n",
       " 'R40': {'type': 'Frequency-Drug', 'arg1': 'T55', 'arg2': 'T50'},\n",
       " 'R41': {'type': 'Duration-Drug', 'arg1': 'T56', 'arg2': 'T50'},\n",
       " 'R42': {'type': 'Strength-Drug', 'arg1': 'T58', 'arg2': 'T57'},\n",
       " 'R43': {'type': 'Form-Drug', 'arg1': 'T59', 'arg2': 'T57'},\n",
       " 'R44': {'type': 'Dosage-Drug', 'arg1': 'T60', 'arg2': 'T57'},\n",
       " 'R45': {'type': 'Form-Drug', 'arg1': 'T61', 'arg2': 'T57'},\n",
       " 'R46': {'type': 'Route-Drug', 'arg1': 'T62', 'arg2': 'T57'},\n",
       " 'R47': {'type': 'Frequency-Drug', 'arg1': 'T63', 'arg2': 'T57'},\n",
       " 'R48': {'type': 'Duration-Drug', 'arg1': 'T64', 'arg2': 'T57'},\n",
       " 'R49': {'type': 'Strength-Drug', 'arg1': 'T66', 'arg2': 'T65'},\n",
       " 'R50': {'type': 'Form-Drug', 'arg1': 'T67', 'arg2': 'T65'},\n",
       " 'R51': {'type': 'Dosage-Drug', 'arg1': 'T68', 'arg2': 'T65'},\n",
       " 'R52': {'type': 'Form-Drug', 'arg1': 'T69', 'arg2': 'T65'},\n",
       " 'R53': {'type': 'Route-Drug', 'arg1': 'T70', 'arg2': 'T65'},\n",
       " 'R54': {'type': 'Frequency-Drug', 'arg1': 'T71', 'arg2': 'T65'},\n",
       " 'R55': {'type': 'Strength-Drug', 'arg1': 'T73', 'arg2': 'T72'},\n",
       " 'R56': {'type': 'Form-Drug', 'arg1': 'T74', 'arg2': 'T72'},\n",
       " 'R57': {'type': 'Dosage-Drug', 'arg1': 'T75', 'arg2': 'T72'},\n",
       " 'R58': {'type': 'Form-Drug', 'arg1': 'T76', 'arg2': 'T72'},\n",
       " 'R59': {'type': 'Route-Drug', 'arg1': 'T77', 'arg2': 'T72'},\n",
       " 'R3': {'type': 'Reason-Drug', 'arg1': 'T5', 'arg2': 'T3'},\n",
       " 'R61': {'type': 'Form-Drug', 'arg1': 'T6', 'arg2': 'T36'},\n",
       " 'R62': {'type': 'Form-Drug', 'arg1': 'T80', 'arg2': 'T43'},\n",
       " 'R29': {'type': 'Frequency-Drug', 'arg1': 'T42', 'arg2': 'T36'},\n",
       " 'R63': {'type': 'Form-Drug', 'arg1': 'T81', 'arg2': 'T57'},\n",
       " 'R64': {'type': 'Form-Drug', 'arg1': 'T82', 'arg2': 'T65'},\n",
       " 'R60': {'type': 'Frequency-Drug', 'arg1': 'T78', 'arg2': 'T72'},\n",
       " 'R65': {'type': 'Form-Drug', 'arg1': 'T83', 'arg2': 'T72'},\n",
       " 'R6': {'type': 'Route-Drug', 'arg1': 'T12', 'arg2': 'T13'},\n",
       " 'R7': {'type': 'Route-Drug', 'arg1': 'T10', 'arg2': 'T11'}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b1f7712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Form',\n",
       " 'start': 5640,\n",
       " 'end': 5670,\n",
       " 'value': 'Tab Sust.Rel. Particle/Crystal'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent['T59']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab5ec87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 427),\n",
       " (428, 645),\n",
       " (646, 684),\n",
       " (685, 768),\n",
       " (770, 879),\n",
       " (880, 909),\n",
       " (910, 960),\n",
       " (961, 990),\n",
       " (992, 1045),\n",
       " (1047, 1161),\n",
       " (1163, 1204),\n",
       " (1205, 1254),\n",
       " (1256, 1304),\n",
       " (1306, 1329),\n",
       " (1330, 1382),\n",
       " (1384, 1422),\n",
       " (1424, 1527),\n",
       " (1528, 1633),\n",
       " (1634, 1681),\n",
       " (1682, 1722),\n",
       " (1723, 1771),\n",
       " (1772, 1962),\n",
       " (1963, 2014),\n",
       " (2015, 2068),\n",
       " (2069, 2119),\n",
       " (2120, 2152),\n",
       " (2153, 2202),\n",
       " (2203, 2281),\n",
       " (2282, 2368),\n",
       " (2369, 2454),\n",
       " (2455, 2492),\n",
       " (2493, 2541),\n",
       " (2542, 2615),\n",
       " (2616, 2648),\n",
       " (2649, 2780),\n",
       " (2784, 3302),\n",
       " (3303, 3369),\n",
       " (3370, 3445),\n",
       " (3446, 3507),\n",
       " (3508, 3612),\n",
       " (3613, 3669),\n",
       " (3670, 3751),\n",
       " (3752, 3831),\n",
       " (3832, 3934),\n",
       " (3935, 4028),\n",
       " (4029, 4119),\n",
       " (4120, 4240),\n",
       " (4241, 4342),\n",
       " (4343, 4409),\n",
       " (4410, 4532),\n",
       " (4533, 4585),\n",
       " (4586, 4640),\n",
       " (4641, 4683),\n",
       " (4685, 4745),\n",
       " (4747, 4770),\n",
       " (4772, 4825),\n",
       " (4827, 4888),\n",
       " (4890, 5053),\n",
       " (5054, 5153),\n",
       " (5154, 5156),\n",
       " (5157, 5278),\n",
       " (5279, 5281),\n",
       " (5282, 5345),\n",
       " (5346, 5380),\n",
       " (5381, 5459),\n",
       " (5460, 5504),\n",
       " (5505, 5587),\n",
       " (5588, 5613),\n",
       " (5614, 5653),\n",
       " (5654, 5697),\n",
       " (5698, 5740),\n",
       " (5741, 5762),\n",
       " (5763, 5798),\n",
       " (5799, 5918),\n",
       " (5919, 5977),\n",
       " (5978, 6046),\n",
       " (6047, 6495),\n",
       " (6496, 6505),\n",
       " (6506, 6536),\n",
       " (6537, 6586),\n",
       " (6587, 6650),\n",
       " (6651, 6722),\n",
       " (6723, 6759),\n",
       " (6762, 7132)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d04324cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "T59",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ent, rel \u001b[39m=\u001b[39m parse_entities_relationships(ann_str)\n\u001b[1;32m----> 2\u001b[0m rel_spans \u001b[39m=\u001b[39m get_relationship_spans(ent, rel, txt_str)\n",
      "Cell \u001b[1;32mIn[38], line 75\u001b[0m, in \u001b[0;36mget_relationship_spans\u001b[1;34m(entities, relationships, txt_str)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_relationship_spans\u001b[39m(entities, relationships, txt_str):\n\u001b[0;32m     72\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m    get relationship spans\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     entities \u001b[39m=\u001b[39m update_entity_indices(entities, sentence_start_end(txt_str))\n\u001b[0;32m     77\u001b[0m     relationship_spans \u001b[39m=\u001b[39m {}\n\u001b[0;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m rel_id, relationship \u001b[39min\u001b[39;00m relationships\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[38], line 67\u001b[0m, in \u001b[0;36mupdate_entity_indices\u001b[1;34m(entities, sent_positions)\u001b[0m\n\u001b[0;32m     65\u001b[0m             entity[\u001b[39m'\u001b[39m\u001b[39msent_end\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m end \u001b[39m-\u001b[39m sent_start\n\u001b[0;32m     66\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m entity, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mentity_key\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m entities\n",
      "\u001b[1;31mAssertionError\u001b[0m: T59"
     ]
    }
   ],
   "source": [
    "rel_spans = get_relationship_spans(ent, rel, txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00bfb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_lst = []\n",
    "df_rel_lst = []\n",
    "tmp = []\n",
    "# for i, (s, v) in enumerate(rel_spans.items()):\n",
    "#     try:\n",
    "#         df1, df2 = get_token_tags(s, v['all_entities'], v['relationships'])\n",
    "#         df1['sid'] = i\n",
    "#         df1['contains_rel'] = 1\n",
    "#         df2['sid'] = i\n",
    "#         df_ner_lst.append(df1)\n",
    "#         df_rel_lst.append(df2)\n",
    "#     except Exception as e:\n",
    "#         print(\"*****\")\n",
    "#         print(e)\n",
    "#         print(\"*****\")\n",
    "#         print(f\"{100035}: {s}\")\n",
    "#         tmp.append((s, v))\n",
    "\n",
    "for i, (s, v) in enumerate(uncovered_ent_spans.items()):\n",
    "    try:\n",
    "        df1 = get_token_tags(s, v)\n",
    "        df1['sid'] = i\n",
    "        df1['contains_rel'] = 0\n",
    "        df_ner_lst.append(df1)\n",
    "    except Exception as e:\n",
    "        print(\"*****\")\n",
    "        print(e)\n",
    "        print(\"*****\")\n",
    "        print(f\"{100035}: {s}\")\n",
    "        tmp.append((s, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a11075c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100035: # Ventilator associated pneumonia:  Patient developed a fever on\n",
      "[**2-27**] with new infiltrates on chest xray while intubated. He was\n",
      "initially covered with vanc/cefepime and cipro.\n",
      "100035: He completed an 8 day course of\n",
      "Vanco/Cefepime.\n",
      "100035: 3. acetaminophen 325 mg Tablet Sig: Two (2) Tablet PO Q6H (every\n",
      "6 hours) as needed for pain/fever.\n",
      "100035: 11. acetaminophen 500 mg Tablet Sig: Two (2) Tablet PO TID (3\n",
      "times a day) as needed for pain/fever.\n",
      "100035: Your mental status\n",
      "slowly improved, though you did have 2 seizures, last on [**3-18**]. You were started ons eizure medications for this.\n",
      "100039: Brief Hospital Course:\n",
      "38 yo F w/ h/o ALL in remission s/p cord transplant in [**1-13**],\n",
      "anthracycline-induced cardiomyopathy (EF 15-20% [**1-14**]) and\n",
      "recurrent nausea and vomiting who presents with 1 week abd pain,\n",
      "acute on chronic renal failure and new hyperbilirubinemia.\n",
      "100039: # Anthracycline-induced/ GVHD cardiomyopathy:  EF <20% on echo\n",
      "from 2/[**2174**].\n",
      "100039: She was then taken to the Cath lab and placed\n",
      "on a milrinone/lasix gtt and transfered to the CCU.\n",
      "100039: She was then taken to the Cath lab and placed\n",
      "on a milrinone/lasix gtt and transfered to the CCU. Her volume\n",
      "overload slowly improved and her peripheral edema/ascites slowly\n",
      "improved as well.\n",
      "100039: She was sent to the cath lab and started on a\n",
      "milrinone/lasix gtt and transfered to the CCU with a goal\n",
      "diuresis of 1L per day.\n",
      "100039: - underwent phase I induction with daunorubicin, vincristine,\n",
      "dexamethasone, L-asparaginase, MTX; phase II with\n",
      "cyclophosphamide, cytarabine, mercaptopurine, MTX\n",
      "- Bone Marrow Aspirate/Biopsy on [**2172-10-26**] showed no morphologic\n",
      "\n",
      "evidence of residual leukemia\n",
      "- underwent allo double cord blood SCT [**2173-1-11**], course\n",
      "complicated by neutropenic fever and acute skin GVHD\n",
      "\n",
      "OTHER MEDICAL HISTORY:\n",
      "- Embolic stroke in [**3-/2174**] on coumadin\n",
      "- Cardiomyopathy due to early anthracycline-related\n",
      "cardiotoxicity [**10/2172**]\n",
      "- Chronic kidney disease stage III/IV, baseline creatinine\n",
      "~2.0-2.2\n",
      "- Asthma\n",
      "- HTN\n",
      "- Cervical Intraepithelial neoplasia\n",
      "- C-section in [**2165**]\n",
      "\n",
      "\n",
      "Social History:\n",
      "Smoke: never\n",
      "EtOH: Occasional in past, none currently\n",
      "Drugs: Never\n",
      "Lives/works: Single, has two children (ages 7 and 18).\n",
      "100039: We made the following changes to your medications:\n",
      "-Mycophenolate Mofetil 1000mg twice a day was started\n",
      "-Prednisone 60mg daily was started\n",
      "-Coumadin was decreased to 2mg daily\n",
      "-Torsemide was increased to 40mg daily\n",
      "-Please hold your valsartan until you see your cardiologist\n",
      "-Metoprolol succinate 100mg daily was started; please stop\n",
      "carvedilol\n",
      "-Bentyl (dicyclomine) was started for your abdominal pain\n",
      "-Simethicone was started for abdominal discomfort/gas\n",
      "-Your morphine was switched to long-acting Morphine 15mg twice a\n",
      "day\n",
      "-Bactrim single strength, 1 tablet daily, was started to help\n",
      "prevent infection\n",
      "-Acyclovir 400mg twice a day was started to help prevent\n",
      "infection\n",
      "-Allopurinol 100mg daily was started because your uric acid\n",
      "levels were high\n",
      "\n",
      "Weigh yourself every morning, [**Name8 (MD) 138**] MD if weight goes up more\n",
      "than 3 lbs.\n",
      "100039: Admission Date:  [**2174-4-18**]              Discharge Date:   [**2174-5-17**]\n",
      "\n",
      "Date of Birth:  [**2135-11-15**]             Sex:   F\n",
      "\n",
      "Service: MEDICINE\n",
      "\n",
      "Allergies:\n",
      "Prochlorperazine / Heparin Agents\n",
      "\n",
      "Attending:[**First Name3 (LF) 3918**]\n",
      "Chief Complaint:\n",
      "Abdominal Pain\n",
      "\n",
      "Major Surgical or Invasive Procedure:\n",
      "Upper GI series with small bowel follow through\n",
      "Right heart catheterization\n",
      "IR guided paracentesis\n",
      "\n",
      "\n",
      "History of Present Illness:\n",
      "38 yo F w/ h/o ALL in remission s/p cord transplant in [**1-13**],\n",
      "anthracycline-induced cardiomyopathy (EF 15-20% [**1-14**]) and\n",
      "recurrent nausea and vomiting who presents with abdominal pain,\n",
      "N/V x1 week\n",
      "\n",
      "Of note, the pt was admitted here from [**Date range (1) **] with nausea and\n",
      "vomitting of unclear etiology.\n",
      "100187: # Depression/SAD: Continued on Prozac, risperdone, wellbutrin,\n",
      "and klonopin.\n",
      "100187: Tiotropium bromide capsule one cap /day\n",
      "14.\n",
      "100187: Pantoprazole 40/ day\n",
      "16.\n",
      "100187: Warfarin 5 mg/day\n",
      "19.\n",
      "100187: She was readmitted [**2107-1-17**] after she was\n",
      "found to have a multifocal pneumonia and was treated with\n",
      "Levo/Flagyl and Vanco.\n",
      "100187: # Pulmonary embolism/DVTs: She has had multiple PEs and has had\n",
      "one even since the placement of a TrapEase IVC filter. CT during\n",
      "recent previous hospitalization revealed appropriate location of\n",
      "filter and CTA on this admission showed improvement of clot. Admission labwork revealed an INR of 7.9. Coumadin was thus\n",
      "held and reversed with FFP and vitamin K given her history of\n",
      "GIB on anticoagulation.\n",
      "100187: The patient was started back on Levofloxacin/Flagyl.\n",
      "100229: The patient completed an empiric seven-day course of\n",
      "vancomycin/zosyn and ten-day course of flagyl.\n",
      "100229: Elevated LFTs/coagulopathy: Secondary to shock liver versus\n",
      "tylenol toxicity.\n",
      "100229: The patient had a history of tylenol use 3g/day for 5 days prior\n",
      "to admission for fever, body aches.\n",
      "100229: Heparin-induced thrombocytopenia\n",
      ".\n",
      "100564: DMII- dx age 31 but only started glyburide [**10-27**]\n",
      "2.\n",
      "100564: DMII- We held glyburide while he was NPO and maintained on a\n",
      "RISS.\n",
      "100564: Diarrhea- likely related to 5FU.\n",
      "100579: Oxycodone-Acetaminophen 5-325 mg Tablet Sig: One (1) Tablet\n",
      "PO HS (at bedtime) as needed for pain/cramping.\n",
      "100590: Fluticasone-Salmeterol 250-50 mcg/Dose Disk [**Hospital1 **]\n",
      "Tiotropium Bromide 18 mcg capsule daily\n",
      "Albuterol Sulfate 90 mcg 2 puff inh q6hours prn SOB/wheeze\n",
      "Albuterol Sulfate 2.5 mg /3 mL (0.083 %) neb q6 prn SOB\n",
      "Ipratropium Bromide 0.02 % Solution 1 inh q6 prn SOB/wheeze\n",
      "Oxycodone-Acetaminophen 5-325 mg 1-2 Tablets PO Q6H prn pain\n",
      "Aspirin 81md daily\n",
      "\n",
      "\n",
      "Discharge Medications:\n",
      "1.\n",
      "100590: Disp:*1 500 mL bottle* Refills:*0*\n",
      "\n",
      "\n",
      "Discharge Disposition:\n",
      "Extended Care\n",
      "\n",
      "Facility:\n",
      "[**Hospital **] Healthcare Center - [**Location (un) **]\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Primary Diagnosis\n",
      "-Altered mental status secondary to excessive narcotics\n",
      "-Severe chronic obstructive pulmonary disease\n",
      "\n",
      "Secondary Diagnosis\n",
      "-Anxiety\n",
      "-Hypertension\n",
      "-Chronic low back pain\n",
      "-Coronary Artery Disease\n",
      "\n",
      "\n",
      "Discharge Condition:\n",
      "Mental Status: Confused - sometimes.\n",
      "100590: He was given albuterol nebs,\n",
      "ipratropium nebs, 125mg IV Solumedrol, 1g IV Ceftriaxone, 500mg\n",
      "IV Azithromycin and Naloxone .4mg x1 for presumed narcotic\n",
      "induced respiratory distress.\n",
      "100590: # Severe COPD & Hypercarbic Respiratory Distress: Initial\n",
      "desaturations and hypercarbia was felt to be related to\n",
      "narcotic- and benzodiazepine-induced respiratory depression.\n",
      "100677: Considering pericardial and\n",
      "pleural fluid pathology, a subtle gastric or pancreatico/biliary\n",
      "tumor was suspected and the patient was started on\n",
      "gemcitabine/irinotecan.\n",
      "100677: The patient was administered Lasix (40 mg X1) in the ED with\n",
      "subsequent improvement of respiratory function.\n",
      "100677: Admission Date:  [**2181-7-12**]              Discharge Date:   [**2181-7-17**]\n",
      "\n",
      "Date of Birth:  [**2126-6-27**]             Sex:   F\n",
      "\n",
      "Service: MEDICINE\n",
      "\n",
      "Allergies:\n",
      "Patient recorded as having No Known Allergies to Drugs\n",
      "\n",
      "Attending:[**First Name3 (LF) 5552**]\n",
      "Chief Complaint:\n",
      "right flank pain\n",
      "\n",
      "Major Surgical or Invasive Procedure:\n",
      "right sided thoracocentesis (-2200 mL fluid)\n",
      "\n",
      "History of Present Illness:\n",
      "55 YO female with metastatic adenocarcinoma with unknown primary\n",
      "on C2D1 gemcitabine/irinotecan and with malignant pleural\n",
      "effusions presented to [**Hospital1 18**] ED with severe R flank pain,\n",
      "radiating to chest.\n",
      "100677: # DVT/PE - She is s/p IVC filter placement on [**2181-5-30**] s/p DVT of\n",
      "common femoral. She was continued on lovenox therapy.\n",
      "100847: He was given\n",
      "6 units of insulin, 2 liters of IV fluid, a dose of Vancomycin\n",
      "x1 (for cellulitis), and a percocet in the ED and transferred to\n",
      "the MICU for further management of his DKA and infection.\n",
      "100847: Left foot abscess/cellulitis/drug reaction:\n",
      "Mr. [**Known lastname 14611**] presented to his podiatrist on [**2149-2-21**] with two days\n",
      "of nausea, vomiting(clear, non-bilious, non bloody) , productive\n",
      "cough (sputum color not noted), fatigue, and pain and redness of\n",
      "his left foot. He was found to have a draining wound\n",
      "(approximately 1 cm in width x 1 cm in depth) on his left lower\n",
      "leg, just superior to the lateral malleolus with an area of\n",
      "surrounding cellulitis. He as admitted to the ED for IV\n",
      "antibiotics and observation.\n",
      "100847: Left foot abscess/cellulitis/drug reaction:\n",
      "Mr. [**Known lastname 14611**] presented to his podiatrist on [**2149-2-21**] with two days\n",
      "of nausea, vomiting(clear, non-bilious, non bloody) , productive\n",
      "cough (sputum color not noted), fatigue, and pain and redness of\n",
      "his left foot. He was found to have a draining wound\n",
      "(approximately 1 cm in width x 1 cm in depth) on his left lower\n",
      "leg, just superior to the lateral malleolus with an area of\n",
      "surrounding cellulitis. He as admitted to the ED for IV\n",
      "antibiotics and observation. In the ED 2 liters of IV fluid, a\n",
      "dose of Vancomycin x1 (for cellulitis, first dose [**2149-2-22**]), and\n",
      "a percocet and transferred to the MICU for further management.\n",
      "100847: In the ED 2 liters of IV fluid, a\n",
      "dose of Vancomycin x1 (for cellulitis, first dose [**2149-2-22**]), and\n",
      "a percocet and transferred to the MICU for further management.\n",
      "100883: MEDICATIONS ON ADMISSION:  Combivent, Flovent, Lipitor,\n",
      "Aspirin, Prednisone 40 on a taper, Metoprolol 25 b.i.d.,\n",
      "Lisinopril 5 once a day, Calcium, Vitamin B, Colace, Percocet\n",
      "and Protonix.\n",
      "100922: Medications on Admission:\n",
      "Lactulose 30cc tid\n",
      "Atovaquone 750 mg/5 mL 10cc daily\n",
      "Citalopram 20 mg daily\n",
      "Montelukast 10 mg daily\n",
      "Mycophenolate Mofetil 500 mg [**Hospital1 **]\n",
      "Omeprazole 20 mg daily\n",
      "Rifaximin 550 mg [**Hospital1 **]\n",
      "Spironolactone 50mg daily\n",
      "Prednisone 10 mg daily\n",
      "Sucralfate 1 gram QID\n",
      "Tacrolimus 0.5 mg daily\n",
      "Torsemide 15 mg daily\n",
      "Calcium 600 with Vitamin D3 600 mg(1,500mg)-400 unit twice a\n",
      "day.\n",
      "101092: # UTI- Currently has suprapubic catheter in setting of\n",
      "neurogenic bladder and was found to have GNR's 10-100K in urine\n",
      "on [**2185-7-3**]. Species on [**7-3**] was Alcaligenes achromobacter. Patient complained of need to void urine on [**2185-7-11**] several\n",
      "times which was new for him. His suprapubic catheter was changed\n",
      "[**2185-7-13**] by urology. Levofloxacin was started on [**2185-7-11**] per\n",
      "[**2185-7-3**] sensitivities; for total 14 day course (last day is\n",
      "[**7-24**]).\n",
      "101092: # Schizophrenia- Difficult to evaluate mental status. No\n",
      "evidence of responding to internal stimuli. Baseline over past\n",
      "few months [**First Name8 (NamePattern2) **] [**First Name4 (NamePattern1) 3504**] [**Last Name (NamePattern1) **] is that he is able to communicate\n",
      "pain/discomfort but does not have capacity. He is mostly lucid\n",
      "but answers questions in mumbles and broken statements. Patient\n",
      "had court date for guardianship on [**2185-7-19**] which was approved. He sometime says inappropriate comments but not frequently. Patient was ambulatory in [**Month (only) 205**] with assistance per previous [**Hospital1 1501**]. We continued his Depakote, Risperdal, and Remeron.\n",
      "101092: # Pneumonia, MRSA: Treated with 8 day course of Vanc/Zosyn for\n",
      "bilateral patchy infiltrate and found to be MRSA positive on\n",
      "bronchial washings.\n",
      "101092: Warfarin 3 mg PO DAILY16\n",
      "Goal INR [**12-24**] (bridge with lovenox if INR <2)\n",
      "RX *warfarin 3 mg 1 tablet(s) by mouth Daily Disp #*30 Tablet\n",
      "Refills:*3\n",
      "\n",
      "\n",
      "Discharge Disposition:\n",
      "Extended Care\n",
      "\n",
      "Facility:\n",
      "[**First Name4 (NamePattern1) 3504**] [**Last Name (NamePattern1) **] Rehabilitation & Nursing Center - [**Location (un) 538**]\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Pneumonia, Pulmonary Embolism\n",
      "\n",
      "\n",
      "Discharge Condition:\n",
      "Activity Status: Out of Bed with assistance to chair or\n",
      "wheelchair.\n",
      "101215\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 245\u001b[0m, in \u001b[0;36mparse_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m     ent, rel \u001b[39m=\u001b[39m parse_entities_relationships(ann_str)\n\u001b[1;32m--> 245\u001b[0m     rel_spans \u001b[39m=\u001b[39m get_relationship_spans(ent, rel, txt_str)\n\u001b[0;32m    246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[3], line 80\u001b[0m, in \u001b[0;36mget_relationship_spans\u001b[1;34m(entities, relationships, txt_str)\u001b[0m\n\u001b[0;32m     79\u001b[0m arg2 \u001b[39m=\u001b[39m entities[relationship[\u001b[39m'\u001b[39m\u001b[39marg2\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m---> 80\u001b[0m \u001b[39mif\u001b[39;00m arg1[\u001b[39m'\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m arg2[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     81\u001b[0m     sent_start \u001b[39m=\u001b[39m arg1[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentence'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parse_data(\u001b[39m\"\u001b[39;49m\u001b[39m../../data/training/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[3], line 248\u001b[0m, in \u001b[0;36mparse_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    247\u001b[0m     \u001b[39mprint\u001b[39m(idx)\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n\u001b[0;32m    249\u001b[0m df_ner_lst \u001b[39m=\u001b[39m []\n\u001b[0;32m    250\u001b[0m df_rel_lst \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parse_data(\"../../data/training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "28fb9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"../../data/training/100187.ann\"\n",
    "f = \"../../data/training/100187.txt\"\n",
    "with open(a, 'r') as a_f:\n",
    "    ann_str = a_f.read()\n",
    "with open(f, 'r') as f_f:\n",
    "    txt_str = f_f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "178439db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent, rel = parse_entities_relationships(ann_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd290638",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_str[:17279])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc940b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent['T277']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57179199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 'Aggressive diuresis\\nwith IV lasix and diuril were unsuccessful so CVVH was\\ninitiated.'\n",
    "# get_token_tags(t, spans[t]['all_entities'], spans[t]['relationships'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences_without_relationships(sentences, sent_positions, entities, relationships, relationship_spans):\n",
    "    processed_sentences = {entities[rel['arg1']]['sentence'] for rel in relationships.values()}\n",
    "    processed_sentences.update({entities[rel['arg2']]['sentence'] for rel in relationships.values()})\n",
    "    for i, (sent_start, sent_end) in enumerate(sent_positions):\n",
    "        if i not in processed_sentences:\n",
    "            entities_in_sent = [e for e in entities.values() if e['sentence'] == i]\n",
    "            if len(entities_in_sent) > 1:\n",
    "                span = sentences[i]\n",
    "\n",
    "                if span not in relationship_spans:\n",
    "                    relationship_spans[span] = {\n",
    "                        'relationships': [],\n",
    "                        'entities': {}\n",
    "                    }\n",
    "\n",
    "                for j, (e1, e2) in enumerate(zip(entities_in_sent[:-1], entities_in_sent[1:])):\n",
    "                    print(e1['id'], e2['id'])\n",
    "                    relationship_spans[span]['relationships'].append({\n",
    "                        'id': f'NOREL{i}_{j}',\n",
    "                        'type': 'NOREL',\n",
    "                        'arg1': e1['id'],\n",
    "                        'arg2': e2['id']\n",
    "                    })\n",
    "\n",
    "                for e in entities_in_sent:\n",
    "                    relationship_spans[span]['entities'][e['id']] = e\n",
    "    return relationship_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, v in ent.items():\n",
    "    v['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_spans = get_relationship_spans(ent, rel, txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2f908b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Ventilator',\n",
       " 'associated',\n",
       " 'pneumonia',\n",
       " ':',\n",
       " 'Patient',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'fever',\n",
       " 'on',\n",
       " '[',\n",
       " '*',\n",
       " '*',\n",
       " '2-27',\n",
       " '*',\n",
       " '*',\n",
       " ']',\n",
       " 'with',\n",
       " 'new',\n",
       " 'infiltrates',\n",
       " 'on',\n",
       " 'chest',\n",
       " 'xray',\n",
       " 'while',\n",
       " 'intubated',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'initially',\n",
       " 'covered',\n",
       " 'with',\n",
       " 'vanc/cefepime',\n",
       " 'and',\n",
       " 'cipro',\n",
       " '.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b9954ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Ventilator associated pneumonia:  Patient developed a fever on\\n[**2-27**] with new infiltrates on chest xray while intubated. He was\\ninitially covered with vanc/cefepime and cipro.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b4d70688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cefepim'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[162:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "abe9089e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m df_rel_lst \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, (s, v) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(rel_spans\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m----> 4\u001b[0m     df1, df2 \u001b[39m=\u001b[39m get_token_tags(s, v[\u001b[39m'\u001b[39;49m\u001b[39mall_entities\u001b[39;49m\u001b[39m'\u001b[39;49m], v[\u001b[39m'\u001b[39;49m\u001b[39mrelationships\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      5\u001b[0m     df1[\u001b[39m'\u001b[39m\u001b[39msid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m i\n\u001b[0;32m      6\u001b[0m     df1[\u001b[39m'\u001b[39m\u001b[39mcontains_rel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[98], line 195\u001b[0m, in \u001b[0;36mget_token_tags\u001b[1;34m(text, all_ents, all_rels)\u001b[0m\n\u001b[0;32m    193\u001b[0m ent_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m    194\u001b[0m \u001b[39mfor\u001b[39;00m eid, val \u001b[39min\u001b[39;00m all_ents\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 195\u001b[0m     tmp, s_idx, e_idx \u001b[39m=\u001b[39m tag_index(\n\u001b[0;32m    196\u001b[0m         text, val[\u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m], val[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m], val[\u001b[39m'\u001b[39;49m\u001b[39mstart_pos\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m     tag_dict \u001b[39m=\u001b[39m tag_dict \u001b[39m|\u001b[39m tmp\n\u001b[0;32m    199\u001b[0m     ent_dict[eid] \u001b[39m=\u001b[39m {\n\u001b[0;32m    200\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstart_idx\u001b[39m\u001b[39m'\u001b[39m: s_idx,\n\u001b[0;32m    201\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mend_idx\u001b[39m\u001b[39m'\u001b[39m: e_idx\n\u001b[0;32m    202\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[98], line 172\u001b[0m, in \u001b[0;36mtag_index\u001b[1;34m(text, ent_type, ent_text, ent_loc)\u001b[0m\n\u001b[0;32m    170\u001b[0m ent_tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(ent_text)\n\u001b[0;32m    171\u001b[0m possible_loc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(tokens\u001b[39m==\u001b[39ment_tokens[\u001b[39m0\u001b[39m])[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 172\u001b[0m ent_start_loc \u001b[39m=\u001b[39m possible_loc[np\u001b[39m.\u001b[39;49margmin(\n\u001b[0;32m    173\u001b[0m     [\u001b[39mabs\u001b[39;49m(ent_loc \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tokens[\u001b[39m0\u001b[39;49m:x]))) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m possible_loc]\n\u001b[0;32m    174\u001b[0m )]\n\u001b[0;32m    175\u001b[0m ent_end_loc \u001b[39m=\u001b[39m ent_start_loc \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(ent_tokens) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ent_tokens) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Meet\\miniconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1338\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[39mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1338\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmin\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Meet\\miniconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Meet\\miniconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "df_ner_lst = []\n",
    "df_rel_lst = []\n",
    "for i, (s, v) in enumerate(rel_spans.items()):\n",
    "    df1, df2 = get_token_tags(s, v['all_entities'], v['relationships'])\n",
    "    df1['sid'] = i\n",
    "    df1['contains_rel'] = 1\n",
    "    df2['sid'] = i\n",
    "    df_ner_lst.append(df1)\n",
    "    df_rel_lst.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3c2c34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R1': {'type': 'Duration-Drug',\n",
       "  'arg1': 'T1',\n",
       "  'arg1_start': 5,\n",
       "  'arg1_end': 5,\n",
       "  'arg2': 'T2',\n",
       "  'arg2_start': 8,\n",
       "  'arg2_end': 8}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_tags(s, rel_spans[s]['all_entities'], rel_spans[s]['relationships'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4c58714",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncovered_ent_spans = get_uncovered_entity_spans(ent, rel, txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "359ecfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The patient had persistent symptoms and was\\nstarted on levofloxacin by his PCP two days prior to admission.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = list(uncovered_ent_spans.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "740a7f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'O'),\n",
       " ('patient', 'O'),\n",
       " ('had', 'O'),\n",
       " ('persistent', 'O'),\n",
       " ('symptoms', 'O'),\n",
       " ('and', 'O'),\n",
       " ('was', 'O'),\n",
       " ('started', 'O'),\n",
       " ('on', 'O'),\n",
       " ('levofloxacin', 'S-Drug'),\n",
       " ('by', 'O'),\n",
       " ('his', 'O'),\n",
       " ('PCP', 'O'),\n",
       " ('two', 'O'),\n",
       " ('days', 'O'),\n",
       " ('prior', 'O'),\n",
       " ('to', 'O'),\n",
       " ('admission', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_tags(s1, uncovered_ent_spans[s1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357efbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "26128e264d228505edcdd079c0afd2b8cc37c9a8b8aeae9abc308e3b38af8b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
