{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDAMUUkXd3dA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy\n",
    "!pip install scispacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JkDvrGDMso1I",
    "outputId": "f6e50e38-0e2b-4434-997f-a8407ad97815"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08jZYlWgRVnP"
   },
   "outputs": [],
   "source": [
    "class Augmentor:\n",
    "    \"\"\"\n",
    "    A class for augmenting tagged text data.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    nlp : spacy.lang.en.English\n",
    "        A spacy language model loaded with the 'en_core_sci_lg' pipeline.\n",
    "    linker : scispacy.linking.EntityLinker\n",
    "        A linker object used for entity resolution.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    augment(df, tag=\"ADE\", score_threshold=0.95)\n",
    "        Augments tagged text data with new entities and returns the augmented DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructs necessary attributes for the Augmentor object.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        nlp : spacy.lang.en.English\n",
    "            A spacy language model loaded with the 'en_core_sci_lg' pipeline.\n",
    "        linker : scispacy.linking.EntityLinker\n",
    "            A linker object used for entity resolution.\n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load(\"en_core_sci_lg\")\n",
    "        self.nlp.add_pipe(\"abbreviation_detector\")\n",
    "        self.nlp.add_pipe(\n",
    "            \"scispacy_linker\",\n",
    "            config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"}\n",
    "        )\n",
    "        self.linker = self.nlp.get_pipe(\"scispacy_linker\")\n",
    "\n",
    "    def augment(self, df, tag=\"ADE\", score_threshold=0.95):\n",
    "        \"\"\"\n",
    "        Augments tagged text data with new entities and returns the augmented DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame containing the tagged text data to be augmented.\n",
    "        tag : str, optional\n",
    "            The tag to be used for the new entities. Default is 'ADE'.\n",
    "        score_threshold : float, optional\n",
    "            The threshold score for selecting candidate entities for augmentation. Default is 0.95.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            A new DataFrame with the augmented data.\n",
    "\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[\"is_augmented\"] = 0\n",
    "        \n",
    "        scu_list = list(df[df[\"tag\"].str.contains(tag)][[\"sid\", \"contains_rel\", \"uid\"]].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "        augmented_data = {\n",
    "            \"token\": [],\n",
    "            \"tag\": [],\n",
    "            \"sid\": [],\n",
    "            \"contains_rel\": [],\n",
    "            \"uid\": [],\n",
    "            \"is_augmented\": []\n",
    "        }\n",
    "\n",
    "        for sid, contains_rel, uid in scu_list:\n",
    "            tokens = list(df[(df[\"sid\"] == sid) & (df[\"contains_rel\"] == contains_rel) & (df[\"uid\"] == uid)][\"token\"])\n",
    "            tags = list(df[(df[\"sid\"] == sid) & (df[\"contains_rel\"] == contains_rel) & (df[\"uid\"] == uid)][\"tag\"])\n",
    "\n",
    "            token_idx_list = []\n",
    "            for i in range(len(tokens)):\n",
    "                if tags[i] == \"B-\" + tag or tags[i] == \"S-\" + tag:\n",
    "                    token_idx_list.append([tokens[i], i, i])\n",
    "                elif tags[i] == \"I-\" + tag or tags[i] == \"E-\" + tag:\n",
    "                    token_idx_list[-1][0] += \" \" + tokens[i]\n",
    "                    token_idx_list[-1][2] = i\n",
    "\n",
    "            for i, [token, s_idx, e_idx] in enumerate(token_idx_list):\n",
    "                doc = self.nlp(token)\n",
    "\n",
    "                if len(doc.ents) == 1 and doc.ents[0].text == token:\n",
    "                    ent = doc.ents[0]\n",
    "                    flag = False\n",
    "\n",
    "                    for cui, score in ent._.kb_ents:\n",
    "                        if score < score_threshold:\n",
    "                            continue\n",
    "\n",
    "                        for alias in set(self.linker.kb.cui_to_entity[cui].aliases):\n",
    "                            # TODO: Add conditions on alias\n",
    "                            if alias != ent.text and len(alias) < 100:\n",
    "                                flag = True\n",
    "                                alias_tokens = nltk.word_tokenize(alias)\n",
    "                                alias_tags = [None for _ in range(len(alias_tokens))]\n",
    "\n",
    "                                if len(alias_tags) == 1:\n",
    "                                    alias_tags[0] = \"S-\" + tag\n",
    "                                else:\n",
    "                                    alias_tags[0] = \"B-\" + tag\n",
    "                                    for j in range(1, len(alias_tags) - 1):\n",
    "                                        alias_tags[j] = \"I-\" + tag\n",
    "                                    alias_tags[-1] = \"E-\" + tag\n",
    "\n",
    "                                aug_tokens = tokens[:s_idx] + alias_tokens + tokens[e_idx + 1:]\n",
    "                                aug_tags = tags[:s_idx] + alias_tags + tags[e_idx + 1:]\n",
    "                                augmented_data[\"token\"] += aug_tokens\n",
    "                                augmented_data[\"tag\"] += aug_tags\n",
    "                                augmented_data[\"sid\"] += [f\"{sid}.{i}\" for _ in range(len(aug_tokens))]\n",
    "                                augmented_data[\"contains_rel\"] += [contains_rel for _ in range(len(aug_tokens))]\n",
    "                                augmented_data[\"uid\"] += [uid for _ in range(len(aug_tokens))]\n",
    "                                augmented_data[\"is_augmented\"] += [1 for _ in range(len(aug_tokens))]\n",
    "                            \n",
    "                            if flag:\n",
    "                                break # TEMP: Only consider one alias\n",
    "                        if flag:                     \n",
    "                            break # Only consider the entity with the highest CUI\n",
    "        \n",
    "        df_augmented = pd.DataFrame.from_dict(augmented_data)\n",
    "\n",
    "        df[\"sid_contains_rel_uid\"] = df.apply(lambda row: str(row[\"sid\"]) + \"_\" + str(row[\"contains_rel\"]) + \"_\" + str(row[\"uid\"]), axis=1)\n",
    "        scu_set = set(str(sid) + \"_\" + str(contains_rel) + \"_\" + str(uid) for (sid, contains_rel, uid) in scu_list)\n",
    "        df_original = df.copy()\n",
    "        df_original = df_original[df_original[\"sid_contains_rel_uid\"].isin(scu_set)]\n",
    "        df_original = df_original.reset_index(drop=True)\n",
    "        df_original = df_original.drop(columns=[\"sid_contains_rel_uid\"])\n",
    "        df_original[\"sid\"] = df_original[\"sid\"].astype(\"string\")\n",
    "\n",
    "        df_original_and_augmented = pd.concat([df_original, df_augmented], ignore_index=True)\n",
    "\n",
    "        return df_original_and_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX4Zbbu-k1Zh"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/content/ner_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nJxr9-0krZD",
    "outputId": "a2a4a610-fe19-483e-8825-f4ca291db8e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/tmp5kjq1g6c\n",
      "Finished download, copying /tmp/tmp5kjq1g6c to cache at /root/.scispacy/datasets/e9f7327283e43f0482f7c0c71b71dec278a58ccb3ffdd03c2c2350159e7ef146.f2a350ad19015b2591545f7feeed6a6d6d2fffcd635d868a5d7fc0dfc3cadfd8.tfidf_vectors_sparse.npz\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/nmslib_index.bin not found in cache, downloading to /tmp/tmpl0ce3xof\n",
      "Finished download, copying /tmp/tmpl0ce3xof to cache at /root/.scispacy/datasets/f48455d6c79262057cce66b4619123c2b558b21092d42fac97f47bb99a5b8f9f.dd70d3dffe7d90d7ac8914460e16a48375dab32485fb6313a34e6fbcaf53218b.nmslib_index.bin\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectorizer.joblib not found in cache, downloading to /tmp/tmpuz9o330w\n",
      "Finished download, copying /tmp/tmpuz9o330w to cache at /root/.scispacy/datasets/8c32f1e7ddf19ec695c321f68a71f06a191aec8efcf6b645b78fa6250d8d81d3.89019b4a62a096f33ea23677557a4cde66ebc8228f30afabac38e32f834020dc.tfidf_vectorizer.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/concept_aliases.json not found in cache, downloading to /tmp/tmpf4z2y_a6\n",
      "Finished download, copying /tmp/tmpf4z2y_a6 to cache at /root/.scispacy/datasets/1428ec15d3b1061731ea273c03699130b3d6b90948993e74bda66af605ff8e2a.aeb7a686c654df6bccb6c2c23d3eda3eb381daaefda4592b58158d0bee53b352.concept_aliases.json\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/kbs/2020-10-09/umls_2020_aa_cat0129.jsonl not found in cache, downloading to /tmp/tmp71l60hs9\n",
      "Finished download, copying /tmp/tmp71l60hs9 to cache at /root/.scispacy/datasets/4d7fb8fcae1035d1e0a47d9072b43d5a628057d35497fbfb2499b4b7b2dd4dd7.05ec7eef12f336d4666da85b7fa69b9401883a7dd4244473f7b88b413ccbba03.umls_2020_aa_cat0129.jsonl\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/umls_semantic_type_tree.tsv not found in cache, downloading to /tmp/tmp1aevc7kq\n",
      "Finished download, copying /tmp/tmp1aevc7kq to cache at /root/.scispacy/datasets/21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv\n"
     ]
    }
   ],
   "source": [
    "augmentor = Augmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufSbL2A6lr6n",
    "outputId": "ce4c6718-ff92-4e66-df96-95cacb99796c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/scispacy/abbreviation.py:230: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n"
     ]
    }
   ],
   "source": [
    "df_original_and_augmented = augmentor.augment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aau-8uU6EIC1"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/flair_ner_train_augmented.txt\", mode=\"w\") as f:\n",
    "    prev_sid = df_original_and_augmented[\"sid\"][0]\n",
    "    for i, row in df_original_and_augmented.iterrows():\n",
    "        if prev_sid != row[\"sid\"]:\n",
    "            prev_sid = row[\"sid\"]\n",
    "            f.write(\"\\n\")\n",
    "        f.write(row[\"token\"] + \" \" + row[\"tag\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5np7FCk6eMjy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
